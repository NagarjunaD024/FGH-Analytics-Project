{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9662984",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this notebook we will use a dedicated environment called \"l2rpn_neurips_2020_track1\", that has 36 substations. Grid2op comes with many different environments, with different problems etc. In this notebook, we will only mention and explain this specific environment.\n",
    "\n",
    "The approach in this notebook is a loss minimization approach. We have used SARSA algorithm with Q-learning network and this new approach is called Deep SARSA Algorithm.\n",
    "\n",
    "The algoritm takes current action $a$ with $\\epsilon$-greedy algorithm on current state $S$. The reward $r$ and next state $S'$ is observed.\n",
    "\n",
    "Later the current state $S$, current action $a$, reward $r$ and next state $S'$ is stored in buffer replay memory. The main idea behind the buffer replay memory is to train q-network on the experiences. The q-network will calculate q-values based on these experiences $Q(S,a)$.\n",
    "\n",
    "A copy of q-network is used as target network to calculate the q-values of next action $a'$ on next state $S'$ where the $a'$ is selected using $\\epsilon$-greedy algorithm. The idea behind this network is that it will predict next q-values $\\hat{Q}(S',a')$ based on next actions $a'$ predicted by $\\epsilon$-greedy algorithm and the next states $S'$. These q-values are then used to calculate next state-action values as $r+\\gamma.\\hat{Q}(S',a')$ which are then used to calculate loss function.\n",
    "\n",
    "The loss function that we used is mean square error loss (MSE) which is calculated as $L = \\frac{1}{|K|}\\sum_{i=1}^{|K|}[(r+\\gamma.\\hat{Q}(S',a'))-Q(S,a)]^2$ Deep SARSA Algorithm will try to minimise this loss by adjusting the weights of the q-network accordingly.\n",
    "\n",
    "The Structure of this notebook is as follows:\n",
    "1. Importing necessary Libraries\n",
    "2. Preprocess environment\n",
    "3. Process replay memory\n",
    "4. Create Deep SARSA Agent\n",
    "5. Evaluate Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1f49d",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f89d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import copy\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import grid2op\n",
    "from grid2op.Exceptions import Grid2OpException\n",
    "from grid2op.Agent import AgentWithConverter\n",
    "from grid2op.Converter import IdToAct\n",
    "from grid2op.MakeEnv import make\n",
    "from grid2op.Runner import Runner\n",
    "\n",
    "from l2rpn_baselines.utils.replayBuffer import ReplayBuffer\n",
    "from l2rpn_baselines.utils.trainingParam import TrainingParam\n",
    "from l2rpn_baselines.utils.save_log_gif import save_log_gif\n",
    "from grid2op.Reward import L2RPNReward\n",
    "from l2rpn_baselines.utils.waring_msgs import _WARN_GPU_MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec778acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from grid2op.Chronics import MultifolderWithCache\n",
    "    _CACHE_AVAILABLE_DEEPQAGENT = True\n",
    "except ImportError:\n",
    "    _CACHE_AVAILABLE_DEEPQAGENT = False\n",
    "\n",
    "try:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        import tensorflow as tf\n",
    "        import tensorflow.keras.optimizers as tfko\n",
    "        from tensorflow.keras.models import Sequential, Model\n",
    "        from tensorflow.keras.layers import Activation, Dense\n",
    "        from tensorflow.keras.layers import Input\n",
    "    _CAN_USE_TENSORFLOW = True\n",
    "except ImportError:\n",
    "    _CAN_USE_TENSORFLOW = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2baff10",
   "metadata": {},
   "source": [
    "## Setting up defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a12248",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LOGS_DIR = \"./logs-eval/dsarsa_baseline\"\n",
    "DEFAULT_NB_EPISODE = 1\n",
    "DEFAULT_NB_PROCESS = 1\n",
    "DEFAULT_MAX_STEPS = -1\n",
    "DEFAULT_NAME = \"Deep_SARSA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cd5c4",
   "metadata": {},
   "source": [
    "## Below is the base class for the deep SARSA Agent \n",
    "This class allows to train and log the training of Deep SARSA algorithm.\n",
    "\n",
    "It derives from :class:`grid2op.Agent.AgentWithConverter` and as such implements the :func:`DeepSARSAAgent.convert_obs` and :func:`DeepSARSAAgent.my_act`\n",
    "\n",
    "It is suppose to be a Baseline, so it implements also the\n",
    "\n",
    "- :func:`DeepSARSAAgent.load`: to load the agent\n",
    "- :func:`DeepSARSAAgent.save`: to save the agent\n",
    "- :func:`DeepSARSAAgent.train`: to train the agent\n",
    "\n",
    "#### Attributes\n",
    "\n",
    "filter_action_fun: ``callable``\n",
    "The function used to filter the action of the action space. See the documentation of grid2op:\n",
    ":class:`grid2op.Converter.IdToAct`\n",
    "`here <https://grid2op.readthedocs.io/en/v0.9.3/converter.html#grid2op.Converter.IdToAct>`_ for more information.\n",
    "\n",
    "replay_buffer: The experience replay buffer\n",
    "\n",
    "deep_sarsa: :class:`BaseDeepSARSA` \n",
    "        The neural network, represented as a :class:`BaseDeepSARSA` object.\n",
    "\n",
    "name: ``str``\n",
    "        The name of the Agent\n",
    "\n",
    "store_action: ``bool``\n",
    "        Whether you want to register which action your agent took or not. Saving the action can slow down a bit\n",
    "        the computation (less than 1%) but can help understand what your agent is doing during its learning process.\n",
    "\n",
    "dict_action: ``str``\n",
    "        The action taken by the agent, represented as a dictionnary. This can be useful to know which type of actions\n",
    "        is taken by your agent. Only filled if :attr:DeepSARSAAgent.store_action` is ``True`\n",
    "\n",
    "istraining: ``bool``\n",
    "        Whether you are training this agent or not. No more really used. Mainly used for backward compatibility.\n",
    "\n",
    "epsilon: ``float``\n",
    "        The epsilon greedy exploration parameter.\n",
    "\n",
    "nb_injection: ``int``\n",
    "        Number of action tagged as \"injection\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "nb_voltage: ``int``\n",
    "        Number of action tagged as \"voltage\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "nb_topology: ``int``\n",
    "        Number of action tagged as \"topology\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "nb_redispatching: ``int``\n",
    "        Number of action tagged as \"redispatching\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "nb_storage: ``int``\n",
    "        Number of action tagged as \"storage\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "        \n",
    "nb_curtail: ``int``\n",
    "        Number of action tagged as \"curtailment\". See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "nb_do_nothing: ``int``\n",
    "        Number of action tagged as \"do_nothing\", *ie* when an action is not modifiying the state of the grid. See the\n",
    "        `official grid2op documentation <https://grid2op.readthedocs.io/en/v0.9.3/action.html?highlight=get_types#grid2op.Action.BaseAction.get_types>`_\n",
    "        for more information.\n",
    "\n",
    "verbose: ``bool``\n",
    "        An effort will be made on the logging (outside of trensorboard) of the training. For now: verbose=True will\n",
    "        allow some printing on the command prompt, and verbose=False will drastically reduce the amount of information\n",
    "        printed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "083238b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSARSAAgent(AgentWithConverter):\n",
    "    def __init__(self,\n",
    "                 action_space,\n",
    "                 nn_archi,\n",
    "                 name=\"DeepSARSAAgent\",\n",
    "                 store_action=True,\n",
    "                 istraining=False,\n",
    "                 filter_action_fun=None,\n",
    "                 verbose=False,\n",
    "                 observation_space=None,\n",
    "                 **kwargs_converters):\n",
    "        if not _CAN_USE_TENSORFLOW:\n",
    "            raise RuntimeError(\"Cannot import tensorflow, this function cannot be used.\")\n",
    "        \n",
    "        AgentWithConverter.__init__(self, action_space, action_space_converter=IdToAct, **kwargs_converters)\n",
    "        self.filter_action_fun = filter_action_fun\n",
    "        if self.filter_action_fun is not None:\n",
    "            self.action_space.filter_action(self.filter_action_fun)\n",
    "\n",
    "        # and now back to the origin implementation\n",
    "        self.replay_buffer = None\n",
    "        self.__nb_env = None\n",
    "\n",
    "        self.deep_sarsa = None\n",
    "        self._training_param = None\n",
    "        self._tf_writer = None\n",
    "        self.name = name\n",
    "        self._losses = None\n",
    "        self.__graph_saved = False\n",
    "        self.store_action = store_action\n",
    "        self.dict_action = {}\n",
    "        self.istraining = istraining\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "        # for tensorbaord\n",
    "        self._train_lr = None\n",
    "\n",
    "        self._reset_num = None\n",
    "\n",
    "        self._max_iter_env_ = 1000000\n",
    "        self._curr_iter_env = 0\n",
    "        self._max_reward = 0.\n",
    "\n",
    "        # action type\n",
    "        self.nb_injection = 0\n",
    "        self.nb_voltage = 0\n",
    "        self.nb_topology = 0\n",
    "        self.nb_line = 0\n",
    "        self.nb_redispatching = 0\n",
    "        self.nb_curtail = 0\n",
    "        self.nb_storage = 0\n",
    "        self.nb_do_nothing = 0\n",
    "\n",
    "        # for over sampling the hard scenarios\n",
    "        self._prev_obs_num = 0\n",
    "        self._time_step_lived = None\n",
    "        self._nb_chosen = None\n",
    "        self._proba = None\n",
    "        self._prev_id = 0\n",
    "        # this is for the \"limit the episode length\" depending on your previous success\n",
    "        self._total_sucesses = 0\n",
    "\n",
    "        # neural network architecture\n",
    "        self._nn_archi = nn_archi\n",
    "\n",
    "        # observation tranformers\n",
    "        self._obs_as_vect = None\n",
    "        self._tmp_obs = None\n",
    "        self._indx_obs = None\n",
    "        self.verbose = verbose\n",
    "        if observation_space is None:\n",
    "            pass\n",
    "        else:\n",
    "            self.init_obs_extraction(observation_space)\n",
    "\n",
    "        # for the frequency of action type\n",
    "        self.current_ = 0\n",
    "        self.nb_ = 10\n",
    "        self._nb_this_time = np.zeros((self.nb_, 8), dtype=int)\n",
    "\n",
    "        #\n",
    "        self._vector_size = None\n",
    "        self._actions_per_ksteps = None\n",
    "        self._illegal_actions_per_ksteps = None\n",
    "        self._ambiguous_actions_per_ksteps = None\n",
    "\n",
    "    def _fill_vectors(self, training_param):\n",
    "        self._vector_size  = self.nb_ * training_param.update_tensorboard_freq\n",
    "        self._actions_per_ksteps = np.zeros((self._vector_size, self.action_space.size()), dtype=np.int)\n",
    "        self._illegal_actions_per_ksteps = np.zeros(self._vector_size, dtype=np.int)\n",
    "        self._ambiguous_actions_per_ksteps = np.zeros(self._vector_size, dtype=np.int)\n",
    "\n",
    "    # grid2op.Agent interface\n",
    "    def convert_obs(self, observation):\n",
    "        \"\"\"\n",
    "        Generic way to convert an observation. This transform it to a vector and the select the attributes that were\n",
    "        selected in :attr:`l2rpn_baselines.utils.NNParams.list_attr_obs` (that have been extracted once and for all\n",
    "        in the :attr:`DeepSARSAAgent._indx_obs` vector).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation: :class:`grid2op.Observation.BaseObservation`\n",
    "            The current observation sent by the environment\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _tmp_obs: ``numpy.ndarray``\n",
    "            The observation as vector with only the proper attribute selected (TODO scaling will be available\n",
    "            in future version)\n",
    "\n",
    "        \"\"\"\n",
    "        obs_as_vect = observation.to_vect()\n",
    "        self._tmp_obs[:] = obs_as_vect[self._indx_obs]\n",
    "        return self._tmp_obs\n",
    "\n",
    "    def my_act(self, transformed_observation, reward, done=False):\n",
    "        \"\"\"\n",
    "        This function will return the action (its id) selected by the underlying :attr:`DeepSARSAAgent.deep_sarsa` network.\n",
    "\n",
    "        Before being used, this method require that the :attr:`DeepSARSAAgent.deep_sarsa` is created. To that end a call\n",
    "        to :func:`DeepSARSAAgent.init_deep_sarsa` needs to have been performed (this is automatically done if you use\n",
    "        baseline we provide and their `evaluate` and `train` scripts).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        transformed_observation: ``numpy.ndarray``\n",
    "            The observation, as transformed after :func:`DeepSARSAAgent.convert_obs`\n",
    "\n",
    "        reward: ``float``\n",
    "            The reward of the last time step. Ignored by this method. Here for retro compatibility with openAI\n",
    "            gym interface.\n",
    "\n",
    "        done: ``bool``\n",
    "            Whether the episode is over or not. This is not used, and is only present to be compliant with\n",
    "            open AI gym interface\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        res: ``int``\n",
    "            The id the action taken.\n",
    "\n",
    "        \"\"\"\n",
    "        predict_movement_int, *_ = self.deep_sarsa.predict_movement(transformed_observation,\n",
    "                                                                epsilon=0.0,\n",
    "                                                                training=False)\n",
    "        res = int(predict_movement_int)\n",
    "        self._store_action_played(res)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def get_action_size(action_space, filter_fun, kwargs_converters):\n",
    "        \"\"\"\n",
    "        This function allows to get the size of the action space if we were to built a :class:`DeepSARSAAgent`\n",
    "        with this parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action_space: :class:`grid2op.ActionSpace`\n",
    "            The grid2op action space used.\n",
    "\n",
    "        filter_fun: ``callable``\n",
    "            see :attr:`DeepSARSAAgent.filter_fun` for more information\n",
    "\n",
    "        kwargs_converters: ``dict``\n",
    "            see the documentation of grid2op for more information:\n",
    "            `here <https://grid2op.readthedocs.io/en/v0.9.3/converter.html?highlight=idToAct#grid2op.Converter.IdToAct.init_converter>`_\n",
    "\n",
    "        \"\"\"\n",
    "        converter = IdToAct(action_space)\n",
    "        converter.init_converter(**kwargs_converters)\n",
    "        if filter_fun is not None:\n",
    "            converter.filter_action(filter_fun)\n",
    "        return converter.n\n",
    "\n",
    "    def init_obs_extraction(self, observation_space):\n",
    "        \"\"\"\n",
    "        This method should be called to initialize the observation (feed as a vector in the neural network)\n",
    "        from its description as a list of its attribute names.\n",
    "        \"\"\"\n",
    "        tmp = np.zeros(0, dtype=np.uint)  # TODO platform independant\n",
    "        for obs_attr_name in self._nn_archi.get_obs_attr():\n",
    "            beg_, end_, dtype_ = observation_space.get_indx_extract(obs_attr_name)\n",
    "            tmp = np.concatenate((tmp, np.arange(beg_, end_, dtype=np.uint)))\n",
    "        self._indx_obs = tmp\n",
    "        self._tmp_obs = np.zeros((1, tmp.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # baseline interface\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Part of the l2rpn_baselines interface, this function allows to read back a trained model, to continue the\n",
    "        training or to evaluate its performance for example.\n",
    "\n",
    "        **NB** To reload an agent, it must have exactly the same name and have been saved at the right location.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: ``str``\n",
    "            The path where the agent has previously beens saved.\n",
    "\n",
    "        \"\"\"\n",
    "        # not modified compare to original implementation\n",
    "        tmp_me = os.path.join(path, self.name)\n",
    "        if not os.path.exists(tmp_me):\n",
    "            raise RuntimeError(\"The model should be stored in \\\"{}\\\". But this appears to be empty\".format(tmp_me))\n",
    "        self._load_action_space(tmp_me)\n",
    "\n",
    "        # TODO handle case where training param class has been overidden\n",
    "        self._training_param = TrainingParam.from_json(os.path.join(tmp_me, \"training_params.json\".format(self.name)))\n",
    "        self.deep_sarsa = self._nn_archi.make_nn(self._training_param)\n",
    "        try:\n",
    "            self.deep_sarsa.load_network(tmp_me, name=self.name)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Impossible to load the model located at \\\"{}\\\" with error \\n{}\".format(path, e))\n",
    "\n",
    "        for nm_attr in [\"_time_step_lived\", \"_nb_chosen\", \"_proba\"]:\n",
    "            conv_path = os.path.join(tmp_me, \"{}.npy\".format(nm_attr))\n",
    "            if os.path.exists(conv_path):\n",
    "                setattr(self, nm_attr, np.load(file=conv_path))\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Part of the l2rpn_baselines interface, this allows to save a model. Its name is used at saving time. The\n",
    "        same name must be reused when loading it back.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: ``str``\n",
    "            The path where to save the agent.\n",
    "\n",
    "        \"\"\"\n",
    "        if path is not None:\n",
    "            tmp_me = os.path.join(path, self.name)\n",
    "            if not os.path.exists(tmp_me):\n",
    "                os.mkdir(tmp_me)\n",
    "            nm_conv = \"action_space.npy\"\n",
    "            conv_path = os.path.join(tmp_me, nm_conv)\n",
    "            if not os.path.exists(conv_path):\n",
    "                self.action_space.save(path=tmp_me, name=nm_conv)\n",
    "\n",
    "            self._training_param.save_as_json(tmp_me, name=\"training_params.json\")\n",
    "            self._nn_archi.save_as_json(tmp_me, \"nn_architecture.json\")\n",
    "            self.deep_sarsa.save_network(tmp_me, name=self.name)\n",
    "\n",
    "            # TODO save the \"oversampling\" part, and all the other info\n",
    "            for nm_attr in [\"_time_step_lived\", \"_nb_chosen\", \"_proba\"]:\n",
    "                conv_path = os.path.join(tmp_me, \"{}.npy\".format(nm_attr))\n",
    "                attr_ = getattr(self, nm_attr)\n",
    "                if attr_ is not None:\n",
    "                    np.save(arr=attr_, file=conv_path)\n",
    "\n",
    "    def train(self,\n",
    "              env,\n",
    "              iterations,\n",
    "              save_path,\n",
    "              logdir,\n",
    "              training_param=None):\n",
    "        \"\"\"\n",
    "        This function allows to train the baseline.\n",
    "\n",
    "        If `save_path` is not None, the the model is saved regularly, and also at the end of training.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: :class:`grid2op.Environment.Environment` or :class:`grid2op.Environment.MultiEnvironment`\n",
    "            The environment used to train your model.\n",
    "\n",
    "        iterations: ``int``\n",
    "            The number of training iteration. NB when reloading a model, this is **NOT** the training steps that will\n",
    "            be used when re training. Indeed, if `iterations` is 1000 and the model was already trained for 750 time\n",
    "            steps, then when reloaded, the training will occur on 250 (=1000 - 750) time steps only.\n",
    "\n",
    "        save_path: ``str``\n",
    "            Location at which to save the model\n",
    "\n",
    "        logdir: ``str``\n",
    "            Location at which tensorboard related information will be kept.\n",
    "\n",
    "        training_param: :class:`l2rpn_baselines.utils.TrainingParam`\n",
    "            The meta parameters for the training procedure. This is currently ignored if the model is reloaded (in that\n",
    "            case the parameters used when first created will be used)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if training_param is None:\n",
    "            training_param = TrainingParam()\n",
    "\n",
    "        self._train_lr = training_param.lr\n",
    "\n",
    "        if self._training_param is None:\n",
    "            self._training_param = training_param\n",
    "        else:\n",
    "            training_param = self._training_param\n",
    "        self._init_deep_sarsa(self._training_param, env)\n",
    "        self._fill_vectors(self._training_param)\n",
    "\n",
    "        self._init_replay_buffer()\n",
    "\n",
    "        # efficient reading of the data (read them by chunk of roughly 1 day\n",
    "        nb_ts_one_day = 24 * 60 / 5  # number of time steps per day\n",
    "        self._set_chunk(env, nb_ts_one_day)\n",
    "\n",
    "        # Create file system related vars\n",
    "        if save_path is not None:\n",
    "            save_path = os.path.abspath(save_path)\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        if logdir is not None:\n",
    "            logpath = os.path.join(logdir, self.name)\n",
    "            self._tf_writer = tf.summary.create_file_writer(logpath, name=self.name)\n",
    "        else:\n",
    "            logpath = None\n",
    "            self._tf_writer = None\n",
    "        UPDATE_FREQ = training_param.update_tensorboard_freq  # update tensorboard every \"UPDATE_FREQ\" steps\n",
    "        SAVING_NUM = training_param.save_model_each\n",
    "\n",
    "        if hasattr(env, \"nb_env\"):\n",
    "            nb_env = env.nb_env\n",
    "            warnings.warn(\"Training using {} environments\".format(nb_env))\n",
    "            self.__nb_env = nb_env\n",
    "        else:\n",
    "            self.__nb_env = 1\n",
    "\n",
    "        self.init_obs_extraction(env.observation_space)\n",
    "\n",
    "        training_step = self._training_param.last_step\n",
    "        self.epsilon = self._training_param.initial_epsilon\n",
    "\n",
    "        # now the number of alive frames and total reward depends on the \"underlying environment\". It is vector instead\n",
    "        # of scalar\n",
    "        alive_frame, total_reward = self._init_global_train_loop()\n",
    "        reward, done = self._init_local_train_loop()\n",
    "        epoch_num = 0\n",
    "        self._losses = np.zeros(iterations)\n",
    "        alive_frames = np.zeros(iterations)\n",
    "        total_rewards = np.zeros(iterations)\n",
    "        new_state = None\n",
    "        self._reset_num = 0\n",
    "        self._curr_iter_env = 0\n",
    "        self._max_reward = env.reward_range[1]\n",
    "\n",
    "        # action types\n",
    "        # injection, voltage, topology, line, redispatching = action.get_types()\n",
    "        self.nb_injection = 0\n",
    "        self.nb_voltage = 0\n",
    "        self.nb_topology = 0\n",
    "        self.nb_line = 0\n",
    "        self.nb_redispatching = 0\n",
    "        self.nb_curtail = 0\n",
    "        self.nb_storage = 0\n",
    "        self.nb_do_nothing = 0\n",
    "\n",
    "        # for non uniform random sampling of the scenarios\n",
    "        th_size = None\n",
    "        self._prev_obs_num = 0\n",
    "        if self.__nb_env == 1:\n",
    "            if _CACHE_AVAILABLE_DEEPQAGENT:\n",
    "                if isinstance(env.chronics_handler.real_data, MultifolderWithCache):\n",
    "                    th_size = env.chronics_handler.real_data.cache_size\n",
    "            if th_size is None:\n",
    "                th_size = len(env.chronics_handler.real_data.subpaths)\n",
    "\n",
    "            # number of time step lived per possible scenarios\n",
    "            if self._time_step_lived is None or self._time_step_lived.shape[0] != th_size:\n",
    "                self._time_step_lived = np.zeros(th_size, dtype=np.uint64)\n",
    "            # number of time a given scenario has been played\n",
    "            if self._nb_chosen is None or self._nb_chosen.shape[0] != th_size:\n",
    "                self._nb_chosen = np.zeros(th_size, dtype=np.uint)\n",
    "            # number of time a given scenario has been played\n",
    "            if self._proba is None or self._proba.shape[0] != th_size:\n",
    "                self._proba = np.ones(th_size, dtype=np.float64)\n",
    "\n",
    "        self._prev_id = 0\n",
    "        # this is for the \"limit the episode length\" depending on your previous success\n",
    "        self._total_sucesses = 0\n",
    "\n",
    "        with tqdm(total=iterations - training_step, disable=not self.verbose) as pbar:\n",
    "            while training_step < iterations:\n",
    "                # reset or build the environment\n",
    "                initial_state = self._need_reset(env, training_step, epoch_num, done, new_state)\n",
    "\n",
    "                # Slowly decay the exploration parameter epsilon\n",
    "                # if self.epsilon > training_param.FINAL_EPSILON:\n",
    "                self.epsilon = self._training_param.get_next_epsilon(current_step=training_step)\n",
    "\n",
    "                # then we need to predict the next moves. Agents have been adapted to predict a batch of data\n",
    "                pm_i, pq_v, act = self._next_move(initial_state, self.epsilon, training_step)\n",
    "                EPS = self.epsilon\n",
    "\n",
    "                # todo store the illegal / ambiguous / ... actions\n",
    "                reward, done = self._init_local_train_loop()\n",
    "                if self.__nb_env == 1:\n",
    "                    act = act[0]\n",
    "\n",
    "                temp_observation_obj, temp_reward, temp_done, info = env.step(act)\n",
    "                if self.__nb_env == 1:\n",
    "                    temp_observation_obj = [temp_observation_obj]\n",
    "                    temp_reward = np.array([temp_reward], dtype=np.float32)\n",
    "                    temp_done = np.array([temp_done], dtype=np.bool)\n",
    "                    info = [info]\n",
    "\n",
    "                new_state = self._convert_obs_train(temp_observation_obj)\n",
    "                self._updage_illegal_ambiguous(training_step, info)\n",
    "                done, reward, total_reward, alive_frame, epoch_num \\\n",
    "                    = self._update_loop(done, temp_reward, temp_done, alive_frame, total_reward, reward, epoch_num)\n",
    "\n",
    "                # update the replay buffer\n",
    "                self._store_new_state(initial_state, pm_i, reward, done, new_state)\n",
    "                \n",
    "                # now train the model\n",
    "                if not self._train_model(training_step):\n",
    "                    # infinite loss in this case\n",
    "                    raise RuntimeError(\"ERROR INFINITE LOSS\")\n",
    "\n",
    "                # Save the network every 1000 iterations\n",
    "                if training_step % SAVING_NUM == 0 or training_step == iterations - 1:\n",
    "                    self.save(save_path)\n",
    "\n",
    "                # save some information to tensorboard\n",
    "                alive_frames[epoch_num] = np.mean(alive_frame)\n",
    "                total_rewards[epoch_num] = np.mean(total_reward)\n",
    "                self._store_action_played_train(training_step, pm_i)\n",
    "                self._save_tensorboard(training_step, epoch_num, UPDATE_FREQ, total_rewards, alive_frames)\n",
    "                training_step += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "        self.save(save_path)\n",
    "\n",
    "    # auxiliary functions\n",
    "    # two below function: to train with multiple environments\n",
    "    def _convert_obs_train(self, observations):\n",
    "        \"\"\" create the observations that are used for training.\"\"\"\n",
    "        if self._obs_as_vect is None:\n",
    "            size_obs = self.convert_obs(observations[0]).shape[1]\n",
    "            self._obs_as_vect = np.zeros((self.__nb_env, size_obs), dtype=np.float32)\n",
    "\n",
    "        for i, obs in enumerate(observations):\n",
    "            self._obs_as_vect[i, :] = self.convert_obs(obs).reshape(-1)\n",
    "        return self._obs_as_vect\n",
    "\n",
    "    def _create_action_if_not_registered(self, action_int):\n",
    "        \"\"\"make sure that `action_int` is present in dict_action\"\"\"\n",
    "        if action_int not in self.dict_action:\n",
    "            act = self.action_space.all_actions[action_int]\n",
    "            is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage, is_dn, is_curtail = \\\n",
    "                False, False, False, False, False, False, False, False\n",
    "            try:\n",
    "                # feature unavailble in grid2op <= 0.9.2\n",
    "                try:\n",
    "                    # storage introduced in grid2op 1.5.0 so if below it is not supported\n",
    "                    is_inj, is_volt, is_topo, is_line_status, is_redisp = act.get_types()\n",
    "                except ValueError as exc_:\n",
    "                    try:\n",
    "                        is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage = act.get_types()\n",
    "                    except ValueError as exc_:\n",
    "                        is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage, is_curtail = act.get_types()\n",
    "\n",
    "                is_dn = (not is_inj) and (not is_volt) and (not is_topo) and (not is_line_status) and (not is_redisp)\n",
    "                is_dn = is_dn and (not is_storage)\n",
    "                is_dn = is_dn and (not is_curtail)\n",
    "            except Exception as exc_:\n",
    "                pass\n",
    "\n",
    "            self.dict_action[action_int] = [0, act,\n",
    "                                            (is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage, is_curtail, is_dn)]\n",
    "\n",
    "    def _store_action_played(self, action_int):\n",
    "        \"\"\"if activated, this function will store the action taken by the agent.\"\"\"\n",
    "        if self.store_action:\n",
    "            self._create_action_if_not_registered(action_int)\n",
    "\n",
    "            self.dict_action[action_int][0] += 1\n",
    "            (is_inj, is_volt, is_topo, is_line_status, is_redisp, is_storage, is_curtail, is_dn) = self.dict_action[action_int][2]\n",
    "            if is_inj:\n",
    "                self.nb_injection += 1\n",
    "            if is_volt:\n",
    "                self.nb_voltage += 1\n",
    "            if is_topo:\n",
    "                self.nb_topology += 1\n",
    "            if is_line_status:\n",
    "                self.nb_line += 1\n",
    "            if is_redisp:\n",
    "                self.nb_redispatching += 1\n",
    "            if is_storage:\n",
    "                self.nb_storage += 1\n",
    "                self.nb_redispatching += 1\n",
    "            if is_curtail:\n",
    "                self.nb_curtail += 1\n",
    "            if is_dn:\n",
    "                self.nb_do_nothing += 1\n",
    "\n",
    "    def _convert_all_act(self, act_as_integer):\n",
    "        \"\"\"this function converts the action given as a list of integer. It ouputs a list of valid grid2op Action\"\"\"\n",
    "        res = []\n",
    "        for act_id in act_as_integer:\n",
    "            res.append(self.convert_act(act_id))\n",
    "            self._store_action_played(act_id)\n",
    "        return res\n",
    "\n",
    "    def _load_action_space(self, path):\n",
    "        \"\"\" load the action space in case the model is reloaded\"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            raise RuntimeError(\"The model should be stored in \\\"{}\\\". But this appears to be empty\".format(path))\n",
    "        try:\n",
    "            self.action_space.init_converter(\n",
    "                all_actions=os.path.join(path, \"action_space.npy\".format(self.name)))\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Impossible to reload converter action space with error \\n{}\".format(e))\n",
    "\n",
    "    # utilities for data reading\n",
    "    def _set_chunk(self, env, nb):\n",
    "        \"\"\"\n",
    "        to optimize the data reading process. See the official grid2op documentation for the effect of setting\n",
    "        the chunk size for the environment.\n",
    "        \"\"\"\n",
    "        env.set_chunk_size(int(max(100, nb)))\n",
    "\n",
    "    def _train_model(self, training_step):\n",
    "        \"\"\"train the deep sarsa networks.\"\"\"\n",
    "        self._training_param.tell_step(training_step)\n",
    "        if training_step > max(self._training_param.min_observation, self._training_param.minibatch_size) and \\\n",
    "            self._training_param.do_train():\n",
    "\n",
    "            # train the model\n",
    "            s_batch, a_batch, r_batch, d_batch, s2_batch = self.replay_buffer.sample(self._training_param.minibatch_size)\n",
    "            tf_writer = None\n",
    "            if self.__graph_saved is False:\n",
    "                tf_writer = self._tf_writer\n",
    "            \n",
    "            loss = self.deep_sarsa.train(s_batch, a_batch, r_batch, d_batch, s2_batch, self.epsilon, \n",
    "                                     tf_writer)\n",
    "            # save learning rate for later\n",
    "            self._train_lr = self.deep_sarsa._optimizer_model._decayed_lr('float32').numpy()\n",
    "            self.__graph_saved = True\n",
    "            if not np.all(np.isfinite(loss)):\n",
    "                # if the loss is not finite i stop the learning\n",
    "                return False\n",
    "            self.deep_sarsa.target_train()\n",
    "            self._losses[training_step:] = np.sum(loss)\n",
    "        return True\n",
    "\n",
    "    def _updage_illegal_ambiguous(self, curr_step, info):\n",
    "        \"\"\"update the conunt of illegal and ambiguous actions\"\"\"\n",
    "        tmp_ = curr_step % self._vector_size\n",
    "        self._illegal_actions_per_ksteps[tmp_] = np.sum([el[\"is_illegal\"] for el in info])\n",
    "        self._ambiguous_actions_per_ksteps[tmp_] = np.sum([el[\"is_ambiguous\"] for el in info])\n",
    "\n",
    "    def _store_action_played_train(self, training_step, action_id):\n",
    "        \"\"\"store which action were played, for tensorboard only.\"\"\"\n",
    "        which_row = training_step % self._vector_size\n",
    "        self._actions_per_ksteps[which_row, :] = 0\n",
    "        self._actions_per_ksteps[which_row, action_id] += 1\n",
    "\n",
    "    def _fast_forward_env(self, env, time=7*24*60/5):\n",
    "        \"\"\"use this functio to skip some time steps when environment is reset.\"\"\"\n",
    "        my_int = np.random.randint(0, min(time, env.chronics_handler.max_timestep()))\n",
    "        env.fast_forward_chronics(my_int)\n",
    "\n",
    "    def _reset_env_clean_state(self, env):\n",
    "        \"\"\"\n",
    "        reset this environment to a proper state. This should rather be integrated in grid2op. And will probably\n",
    "        be integrated partially starting from grid2op 1.0.0\n",
    "        \"\"\"\n",
    "        # /!\\ DO NOT ATTEMPT TO MODIFY OTHERWISE IT WILL PROBABLY CRASH /!\\\n",
    "        # /!\\ THIS WILL BE PART OF THE ENVIRONMENT IN FUTURE GRID2OP RELEASE (>= 1.0.0) /!\\\n",
    "        # AND OF COURSE USING THIS METHOD DURING THE EVALUATION IS COMPLETELY FORBIDDEN\n",
    "        if self.__nb_env > 1:\n",
    "            return\n",
    "        env.current_obs = None\n",
    "        env.env_modification = None\n",
    "        env._reset_maintenance()\n",
    "        env._reset_redispatching()\n",
    "        env._reset_vectors_and_timings()\n",
    "        _backend_action = env._backend_action_class()\n",
    "        _backend_action.all_changed()\n",
    "        env._backend_action =_backend_action\n",
    "        env.backend.apply_action(_backend_action)\n",
    "        _backend_action.reset()\n",
    "        *_, fail_to_start, info = env.step(env.action_space())\n",
    "        if fail_to_start:\n",
    "            # this is happening because not enough care has been taken to handle these problems\n",
    "            # more care will be taken when this feature will be available in grid2op directly.\n",
    "            raise Grid2OpException(\"Impossible to initialize the powergrid, the powerflow diverge at iteration 0. \"\n",
    "                                   \"Available information are: {}\".format(info))\n",
    "        env._reset_vectors_and_timings()\n",
    "\n",
    "    def _need_reset(self, env, observation_num, epoch_num, done, new_state):\n",
    "        \"\"\"perform the proper reset of the environment\"\"\"\n",
    "        if self._training_param.step_increase_nb_iter is not None and \\\n",
    "           self._training_param.step_increase_nb_iter > 0:\n",
    "            self._max_iter_env(min(max(self._training_param.min_iter,\n",
    "                                       self._training_param.max_iter_fun(self._total_sucesses)),\n",
    "                                   self._training_param.max_iter))  # TODO\n",
    "        self._curr_iter_env += 1\n",
    "        if new_state is None:\n",
    "            # it's the first ever loop\n",
    "            obs = env.reset()\n",
    "            if self.__nb_env == 1:\n",
    "                obs = [obs]\n",
    "            new_state = self._convert_obs_train(obs)\n",
    "        elif self.__nb_env > 1:\n",
    "            pass\n",
    "        elif done[0]:\n",
    "            nb_ts_one_day = 24*60/5\n",
    "            if False:\n",
    "                # the 3-4 lines below allow to reuse the loaded dataset and continue further up\n",
    "                try:\n",
    "                    self._reset_env_clean_state(env)\n",
    "                    # random fast forward between now and next day\n",
    "                    self._fast_forward_env(env, time=nb_ts_one_day)\n",
    "                except (StopIteration, Grid2OpException):\n",
    "                    env.reset()\n",
    "                    # random fast forward between now and next week\n",
    "                    self._fast_forward_env(env, time=7*nb_ts_one_day)\n",
    "\n",
    "            # update the number of time steps it has live\n",
    "            ts_lived = observation_num - self._prev_obs_num\n",
    "            if self._time_step_lived is not None:\n",
    "                self._time_step_lived[self._prev_id] += ts_lived\n",
    "            self._prev_obs_num = observation_num\n",
    "            if self._training_param.oversampling_rate is not None:\n",
    "                # proba = np.sqrt(1. / (self._time_step_lived +1))\n",
    "                # # over sampling some kind of \"UCB like\" stuff\n",
    "                # # https://banditalgs.com/2016/09/18/the-upper-confidence-bound-algorithm/\n",
    "\n",
    "                # proba = 1. / (self._time_step_lived + 1)\n",
    "                self._proba[:] = 1. / (self._time_step_lived ** self._training_param.oversampling_rate + 1)\n",
    "                self._proba /= np.sum(self._proba)\n",
    "\n",
    "            _prev_id = self._prev_id\n",
    "            self._prev_id = None\n",
    "            if _CACHE_AVAILABLE_DEEPQAGENT:\n",
    "                if isinstance(env.chronics_handler.real_data, MultifolderWithCache):\n",
    "                    self._prev_id = env.chronics_handler.real_data.sample_next_chronics(self._proba)\n",
    "            if self._prev_id is None:\n",
    "                self._prev_id = _prev_id + 1\n",
    "                self._prev_id %= self._time_step_lived.shape[0]\n",
    "\n",
    "            obs = self._reset_env(env, epoch_num)\n",
    "            if self._training_param.sample_one_random_action_begin is not None and \\\n",
    "                    observation_num < self._training_param.sample_one_random_action_begin:\n",
    "                done = True\n",
    "                while done:\n",
    "                    act = env.action_space(env.action_space._sample_set_bus())\n",
    "                    obs, reward, done, info = env.step(act)\n",
    "                    if info[\"is_illegal\"] or info[\"is_ambiguous\"]:\n",
    "                        # there are no guarantee that sampled action are legal nor perfectly\n",
    "                        # correct.\n",
    "                        # if that is the case, i \"simply\" restart the process, as if the action\n",
    "                        # broke everything\n",
    "                        done = True\n",
    "\n",
    "                    if done:\n",
    "                        obs = self._reset_env(env, epoch_num)\n",
    "                    else:\n",
    "                        if self.verbose:\n",
    "                            print(\"step {}: {}\".format(observation_num, act))\n",
    "\n",
    "                obs = [obs]\n",
    "            new_state = self._convert_obs_train(obs)\n",
    "        return new_state\n",
    "\n",
    "    def _reset_env(self, env, epoch_num):\n",
    "        env.reset()\n",
    "        if self._nb_chosen is not None:\n",
    "            self._nb_chosen[self._prev_id] += 1\n",
    "\n",
    "        # random fast forward between now and next week\n",
    "        if self._training_param.random_sample_datetime_start is not None:\n",
    "            self._fast_forward_env(env, time=self._training_param.random_sample_datetime_start)\n",
    "\n",
    "        self._curr_iter_env = 0\n",
    "        obs = [env.current_obs]\n",
    "        if epoch_num % len(env.chronics_handler.real_data.subpaths) == 0:\n",
    "            # re-shuffle the data\n",
    "            env.chronics_handler.shuffle(lambda x: x[np.random.choice(len(x), size=len(x), replace=False)])\n",
    "        return obs\n",
    "\n",
    "    def _init_replay_buffer(self):\n",
    "        \"\"\"create and initialized the replay buffer\"\"\"\n",
    "        self.replay_buffer = ReplayBuffer(self._training_param.buffer_size)\n",
    "\n",
    "    def _store_new_state(self, initial_state, predict_movement_int, reward, done, new_state):\n",
    "        \"\"\"store the new state in the replay buffer\"\"\"\n",
    "        # vectorized version of the previous code\n",
    "        for i_s, pm_i, reward, done, ns in zip(initial_state, predict_movement_int, reward, done, new_state):\n",
    "            self.replay_buffer.add(i_s,\n",
    "                                   pm_i,\n",
    "                                   reward,\n",
    "                                   done,\n",
    "                                   ns)\n",
    "\n",
    "    def _max_iter_env(self, new_max_iter):\n",
    "        \"\"\"update the number of maximum iteration allowed.\"\"\"\n",
    "        self._max_iter_env_ = new_max_iter\n",
    "\n",
    "    def _next_move(self, curr_state, epsilon, training_step):\n",
    "        # supposes that 0 encodes for do nothing, otherwise it will NOT work (for the observer)\n",
    "        pm_i, pq_v, q_actions = self.deep_sarsa.predict_movement(curr_state, epsilon, training=True)\n",
    "        pm_i, pq_v = self._short_circuit_actions(training_step, pm_i, pq_v, q_actions)\n",
    "        act = self._convert_all_act(pm_i)\n",
    "        return pm_i, pq_v, act\n",
    "\n",
    "    def _short_circuit_actions(self, training_step, pm_i, pq_v, q_actions):\n",
    "        if self._training_param.min_observe is not None and \\\n",
    "                training_step < self._training_param.min_observe:\n",
    "            # action is replaced by do nothing due to the \"observe only\" specification\n",
    "            pm_i[:] = 0\n",
    "            pq_v[:] = q_actions[:, 0]\n",
    "        return pm_i, pq_v\n",
    "\n",
    "    def _init_global_train_loop(self):\n",
    "        alive_frame = np.zeros(self.__nb_env, dtype=np.int)\n",
    "        total_reward = np.zeros(self.__nb_env, dtype=np.float32)\n",
    "        return alive_frame, total_reward\n",
    "\n",
    "    def _update_loop(self, done, temp_reward, temp_done, alive_frame, total_reward, reward, epoch_num):\n",
    "        if self.__nb_env == 1:\n",
    "            # force end of episode at early stage of learning\n",
    "            if self._curr_iter_env >= self._max_iter_env_:\n",
    "                temp_done[0] = True\n",
    "                temp_reward[0] = self._max_reward\n",
    "                self._total_sucesses += 1\n",
    "\n",
    "        done = temp_done\n",
    "        alive_frame[done] = 0\n",
    "        total_reward[done] = 0.\n",
    "        self._reset_num += np.sum(done)\n",
    "        if self._reset_num >= self.__nb_env:\n",
    "            # increase the \"global epoch num\" represented by \"epoch_num\" only when on average\n",
    "            # all environments are \"done\"\n",
    "            epoch_num += 1\n",
    "            self._reset_num = 0\n",
    "\n",
    "        total_reward[~done] += temp_reward[~done]\n",
    "        alive_frame[~done] += 1\n",
    "        return done, temp_reward, total_reward, alive_frame, epoch_num\n",
    "\n",
    "    def _init_local_train_loop(self):\n",
    "        # reward, done = np.zeros(self.nb_process), np.full(self.nb_process, fill_value=False, dtype=np.bool)\n",
    "        reward = np.zeros(self.__nb_env, dtype=np.float32)\n",
    "        done = np.full(self.__nb_env, fill_value=False, dtype=np.bool)\n",
    "        return reward, done\n",
    "\n",
    "    def _init_deep_sarsa(self, training_param, env):\n",
    "        \"\"\"\n",
    "        This function serves as initializin the neural network.\n",
    "        \"\"\"\n",
    "        if self.deep_sarsa is None:\n",
    "            self.deep_sarsa = self._nn_archi.make_nn(training_param)\n",
    "        self.init_obs_extraction(env.observation_space)\n",
    "\n",
    "    def _save_tensorboard(self, step, epoch_num, UPDATE_FREQ, epoch_rewards, epoch_alive):\n",
    "        \"\"\"save all the informations needed in tensorboard.\"\"\"\n",
    "        if self._tf_writer is None:\n",
    "            return\n",
    "\n",
    "        # Log some useful metrics every even updates\n",
    "        if step % UPDATE_FREQ == 0 and epoch_num > 0:\n",
    "            if step % (10 * UPDATE_FREQ) == 0:\n",
    "                # print the top k scenarios the \"hardest\" (ie chosen the most number of times\n",
    "                if self.verbose:\n",
    "                    top_k = 10\n",
    "                    if self._nb_chosen is not None:\n",
    "                        array_ = np.argsort(self._nb_chosen)[-top_k:][::-1]\n",
    "                        print(\"hardest scenarios\\n{}\".format(array_))\n",
    "                        print(\"They have been chosen respectively\\n{}\".format(self._nb_chosen[array_]))\n",
    "                        # print(\"Associated proba are\\n{}\".format(self._proba[array_]))\n",
    "                        print(\"The number of timesteps played is\\n{}\".format(self._time_step_lived[array_]))\n",
    "                        print(\"avg (accross all scenarios) number of timsteps played {}\"\n",
    "                              \"\".format(np.mean(self._time_step_lived)))\n",
    "                        print(\"Time alive: {}\".format(self._time_step_lived[array_] / (self._nb_chosen[array_] + 1)))\n",
    "                        print(\"Avg time alive: {}\".format(np.mean(self._time_step_lived / (self._nb_chosen + 1 ))))\n",
    "\n",
    "            with self._tf_writer.as_default():\n",
    "                last_alive = epoch_alive[(epoch_num-1)]\n",
    "                last_reward = epoch_rewards[(epoch_num-1)]\n",
    "\n",
    "                mean_reward = np.nanmean(epoch_rewards[:epoch_num])\n",
    "                mean_alive = np.nanmean(epoch_alive[:epoch_num])\n",
    "\n",
    "                mean_reward_30 = mean_reward\n",
    "                mean_alive_30 = mean_alive\n",
    "                mean_reward_100 = mean_reward\n",
    "                mean_alive_100 = mean_alive\n",
    "\n",
    "                tmp = self._actions_per_ksteps > 0\n",
    "                tmp = tmp.sum(axis=0)\n",
    "                nb_action_taken_last_kstep = np.sum(tmp > 0)\n",
    "\n",
    "                nb_illegal_act = np.sum(self._illegal_actions_per_ksteps)\n",
    "                nb_ambiguous_act = np.sum(self._ambiguous_actions_per_ksteps)\n",
    "\n",
    "                if epoch_num >= 100:\n",
    "                    mean_reward_100 = np.nanmean(epoch_rewards[(epoch_num-100):epoch_num])\n",
    "                    mean_alive_100 = np.nanmean(epoch_alive[(epoch_num-100):epoch_num])\n",
    "\n",
    "                if epoch_num >= 30:\n",
    "                    mean_reward_30 = np.nanmean(epoch_rewards[(epoch_num-30):epoch_num])\n",
    "                    mean_alive_30 = np.nanmean(epoch_alive[(epoch_num-30):epoch_num])\n",
    "\n",
    "                # to ensure \"fair\" comparison between single env and multi env\n",
    "                step_tb = step  # * self.__nb_env\n",
    "                # if multiply by the number of env we have \"trouble\" with random exploration at the beginning\n",
    "                # because it lasts the same number of \"real\" steps\n",
    "\n",
    "                # show first the Mean reward and mine time alive (hence the upper case)\n",
    "                tf.summary.scalar(\"Mean_alive_30\", mean_alive_30, step_tb,\n",
    "                                  description=\"Average number of steps (per episode) made over the last 30 \"\n",
    "                                              \"completed episodes\")\n",
    "                tf.summary.scalar(\"Mean_reward_30\", mean_reward_30, step_tb,\n",
    "                                  description=\"Average (final) reward obtained over the last 30 completed episodes\")\n",
    "\n",
    "                # then it's alpha numerical order, hence the \"z_\" in front of some information\n",
    "                tf.summary.scalar(\"loss\", self._losses[step], step_tb,\n",
    "                                  description=\"Training loss (for the last training batch)\")\n",
    "\n",
    "                tf.summary.scalar(\"last_alive\", last_alive, step_tb,\n",
    "                                  description=\"Final number of steps for the last complete episode\")\n",
    "                tf.summary.scalar(\"last_reward\", last_reward, step_tb,\n",
    "                                  description=\"Final reward over the last complete episode\")\n",
    "\n",
    "                tf.summary.scalar(\"mean_reward\", mean_reward, step_tb,\n",
    "                                  description=\"Average reward over the whole episodes played\")\n",
    "                tf.summary.scalar(\"mean_alive\", mean_alive, step_tb,\n",
    "                                  description=\"Average time alive over the whole episodes played\")\n",
    "\n",
    "                tf.summary.scalar(\"mean_reward_100\", mean_reward_100, step_tb,\n",
    "                                  description=\"Average number of steps (per episode) made over the last 100 \"\n",
    "                                              \"completed episodes\")\n",
    "                tf.summary.scalar(\"mean_alive_100\", mean_alive_100, step_tb,\n",
    "                                  description=\"Average (final) reward obtained over the last 100 completed episodes\")\n",
    "\n",
    "                tf.summary.scalar(\"nb_different_action_taken\", nb_action_taken_last_kstep, step_tb,\n",
    "                                  description=\"Number of different actions played the last \"\n",
    "                                              \"{} steps\".format(self.nb_ * UPDATE_FREQ))\n",
    "                tf.summary.scalar(\"nb_illegal_act\", nb_illegal_act, step_tb,\n",
    "                                  description=\"Number of illegal actions played the last \"\n",
    "                                              \"{} steps\".format(self.nb_ * UPDATE_FREQ))\n",
    "                tf.summary.scalar(\"nb_ambiguous_act\", nb_ambiguous_act, step_tb,\n",
    "                                  description=\"Number of ambiguous actions played the last \"\n",
    "                                              \"{} steps\".format(self.nb_ * UPDATE_FREQ))\n",
    "                tf.summary.scalar(\"nb_total_success\", self._total_sucesses, step_tb,\n",
    "                                  description=\"Number of times the episode was completed entirely \"\n",
    "                                              \"(no game over)\")\n",
    "\n",
    "                tf.summary.scalar(\"z_lr\", self._train_lr, step_tb,\n",
    "                                  description=\"Current learning rate\")\n",
    "                tf.summary.scalar(\"z_epsilon\", self.epsilon, step_tb,\n",
    "                                  description=\"Current epsilon (from the epsilon greedy)\")\n",
    "                tf.summary.scalar(\"z_max_iter\", self._max_iter_env_, step_tb,\n",
    "                                  description=\"Maximum number of time steps before deciding a scenario \"\n",
    "                                              \"is over (=win)\")\n",
    "                tf.summary.scalar(\"z_total_episode\", epoch_num, step_tb,\n",
    "                                  description=\"Total number of episode played (number of \\\"reset\\\")\")\n",
    "\n",
    "                self.deep_sarsa.save_tensorboard(step_tb)\n",
    "\n",
    "                if self.store_action:\n",
    "                    self._store_frequency_action_type(UPDATE_FREQ, step_tb)\n",
    "\n",
    "                \n",
    "\n",
    "    def _store_frequency_action_type(self, UPDATE_FREQ, step_tb):\n",
    "        self.current_ += 1\n",
    "        self.current_ %= self.nb_\n",
    "        nb_inj, nb_volt, nb_topo, nb_line, nb_redisp, nb_storage, nb_curtail, nb_dn = self._nb_this_time[self.current_, :]\n",
    "        self._nb_this_time[self.current_, :] = [self.nb_injection,\n",
    "                                                self.nb_voltage,\n",
    "                                                self.nb_topology,\n",
    "                                                self.nb_line,\n",
    "                                                self.nb_redispatching,\n",
    "                                                self.nb_storage,\n",
    "                                                self.nb_curtail,\n",
    "                                                self.nb_do_nothing]\n",
    "\n",
    "        curr_inj = self.nb_injection - nb_inj\n",
    "        curr_volt = self.nb_voltage - nb_volt\n",
    "        curr_topo = self.nb_topology - nb_topo\n",
    "        curr_line = self.nb_line - nb_line\n",
    "        curr_redisp = self.nb_redispatching - nb_redisp\n",
    "        curr_storage = self.nb_storage - nb_storage\n",
    "        curr_curtail = self.nb_curtail - nb_curtail\n",
    "        curr_dn = self.nb_do_nothing - nb_dn\n",
    "\n",
    "        total_act_num = curr_inj + curr_volt + curr_topo + curr_line + curr_redisp + curr_dn + curr_storage\n",
    "        tf.summary.scalar(\"zz_freq_inj\",\n",
    "                          curr_inj / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"injection\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"zz_freq_voltage\",\n",
    "                          curr_volt / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"voltage\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_topo\",\n",
    "                          curr_topo / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"topo\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_line_status\",\n",
    "                          curr_line / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"line status\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_redisp\",\n",
    "                          curr_redisp / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"redispatching\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_do_nothing\",\n",
    "                          curr_dn / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"do nothing\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_storage\",\n",
    "                          curr_storage / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"storage\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))\n",
    "        tf.summary.scalar(\"z_freq_curtail\",\n",
    "                          curr_curtail / total_act_num,\n",
    "                          step_tb,\n",
    "                          description=\"Frequency of \\\"curtailment\\\" actions \"\n",
    "                                      \"type played over the last {} actions\"\n",
    "                                      \"\".format(self.nb_ * UPDATE_FREQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc993fa1",
   "metadata": {},
   "source": [
    "## Abstract class to create neural networks\n",
    "\n",
    "This class aims at representing the Q value (or more in case of SAC) parametrization by a neural network.\t\n",
    "        \n",
    "It is composed of 2 different networks:\n",
    "\n",
    "- model: which is the main model\n",
    "- target_model: which has the same architecture and same initial weights as \"model\" but is updated less frequently to stabilize training\n",
    "\n",
    "It has basic methods to make predictions, to train the model, and train the target model.\n",
    "\n",
    "This class is abstraction and need to be overide in order to create object from this class. The only pure virtual function is :func:`BaseDeepSARSA.construct_q_network` that creates the neural network from the nn_params (:class:`NNParam`) provided as input\n",
    "\n",
    "#### Attributes\n",
    "\n",
    "_action_size: ``int``\n",
    "        Total number of actions\n",
    "\n",
    "_observation_size: ``int``\n",
    "        Size of the observation space considered\n",
    "\n",
    "_nn_archi: :class:`NNParam`\n",
    "        The parameters of the neural networks that will be created\n",
    "\n",
    "_training_param: :class:`TrainingParam`\n",
    "        The meta parameters for the training scheme (used especially for learning rate or gradient clipping for example)\n",
    "\n",
    "_lr: ``float``\n",
    "        The  initial learning rate\n",
    "\n",
    "_lr_decay_steps: ``float``\n",
    "        The decay step of the learning rate\n",
    "\n",
    "_lr_decay_rate: ``float``\n",
    "        The rate at which the learning rate will decay\n",
    "\n",
    "_model:\n",
    "        Main neural network model, here a keras Model object.\n",
    "\n",
    "_target_model:\n",
    "        a copy of the main neural network that will be updated less frequently (also known as \"target model\" in RL\n",
    "        community)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0491ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDeepSARSA(ABC):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nn_params,\n",
    "                 training_param=None,\n",
    "                 verbose=False):\n",
    "        if not _CAN_USE_TENSORFLOW:\n",
    "            raise RuntimeError(\"Cannot import tensorflow, this function cannot be used.\")\n",
    "        self._action_size = nn_params.action_size\n",
    "        self._observation_size = nn_params.observation_size\n",
    "        self._nn_archi = nn_params\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if training_param is None:\n",
    "            self._training_param = TrainingParam()\n",
    "        else:\n",
    "            self._training_param = training_param\n",
    "\n",
    "        self._lr = training_param.lr\n",
    "        self._lr_decay_steps = training_param.lr_decay_steps\n",
    "        self._lr_decay_rate = training_param.lr_decay_rate\n",
    "\n",
    "        self._model = None\n",
    "        self._target_model = None\n",
    "        self._schedule_model = None\n",
    "        self._optimizer_model = None\n",
    "        self._custom_objects = None  # to be able to load other keras layers type\n",
    "\n",
    "    def make_optimiser(self):\n",
    "        \"\"\"\n",
    "        helper function to create the proper optimizer (Adam) with the learning rates and its decay\n",
    "        parameters.\n",
    "        \"\"\"\n",
    "        schedule = tfko.schedules.InverseTimeDecay(self._lr, self._lr_decay_steps, self._lr_decay_rate)\n",
    "        return schedule, tfko.Adam(learning_rate=schedule)\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_q_network(self):\n",
    "        \"\"\"\n",
    "         Abstract method that need to be overide.\n",
    "\n",
    "         It should create :attr:`BaseDeepSARSA._model` and :attr:`BaseDeepSARSA._target_model`\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    def predict_movement(self, data, epsilon, batch_size=None, training=False):\n",
    "        \"\"\"\n",
    "        Predict movement of game controler where is epsilon probability randomly move.\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = data.shape[0]\n",
    "\n",
    "        q_actions = self._model(data, training=training).numpy()\n",
    "        opt_policy = np.argmax(q_actions, axis=-1)\n",
    "        if epsilon > 0.:\n",
    "            rand_val = np.random.random(batch_size)\n",
    "            opt_policy[rand_val < epsilon] = np.random.randint(0, self._action_size, size=(np.sum(rand_val < epsilon)))\n",
    "        return opt_policy, q_actions[np.arange(batch_size), opt_policy], q_actions\n",
    "\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, epsilon_tr, tf_writer=None, batch_size=None):\n",
    "        \"\"\"\n",
    "        Trains network to fit given parameters:\n",
    "        \n",
    "        .. seealso::\n",
    "            https://towardsdatascience.com/dueling-double-deep-q-learning-using-tensorflow-2-x-7bbbcec06a2a\n",
    "            for the update rules\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        s_batch:\n",
    "            the state vector (before the action is taken)\n",
    "        a_batch:\n",
    "            the action taken\n",
    "        s2_batch:\n",
    "            the state vector (after the action is taken)\n",
    "        d_batch:\n",
    "            says whether or not the episode was over\n",
    "        r_batch:\n",
    "            the reward obtained this step\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = s_batch.shape[0]\n",
    "\n",
    "        # Save the graph just the first time\n",
    "        if tf_writer is not None:\n",
    "            tf.summary.trace_on()\n",
    "        target = self._model(s_batch, training=True).numpy()\n",
    "        # this fut_action should come from epsilon policy\n",
    "        next_a, fut_actions_3, fut_action_2 = self.predict_movement(s2_batch,epsilon=epsilon_tr,training=True)\n",
    "        fut_action = self._model(s2_batch, training=True).numpy()\n",
    "        \n",
    "\n",
    "        if tf_writer is not None:\n",
    "            with tf_writer.as_default():\n",
    "                tf.summary.trace_export(\"model-graph\", 0)\n",
    "            tf.summary.trace_off()\n",
    "        target_next = self._target_model(s2_batch, training=True).numpy()\n",
    "\n",
    "        idx = np.arange(batch_size)\n",
    "        target[idx, a_batch] = r_batch\n",
    "        # update the value for not done episode\n",
    "        nd_batch = ~d_batch  # update with this rule only batch that did not game over\n",
    "        next_action = np.argmax(fut_action, axis=-1)  # compute the future action i will take in the next state\n",
    "        fut_Q = target_next[idx, next_a]  # get its Q value\n",
    "        fut_Q_new = target_next[idx, next_action]  # get its Q value\n",
    "        \n",
    "        target[nd_batch, a_batch[nd_batch]] += self._training_param.discount_factor * fut_Q[nd_batch]\n",
    "        loss = self.train_on_batch(self._model, self._optimizer_model, s_batch, target)\n",
    "        return loss\n",
    "\n",
    "    def train_on_batch(self, model, optimizer_model, x, y_true):\n",
    "        \"\"\"train the model on a batch of example. This can be overide\"\"\"\n",
    "        loss = model.train_on_batch(x, y_true)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def get_path_model(path, name=None):\n",
    "        \"\"\"\n",
    "        Get the location at which the neural networks will be saved.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        path_model: ``str``\n",
    "            The path at which the model will be saved (path include both path and name, it is the full path at which\n",
    "            the neural networks are saved)\n",
    "\n",
    "        path_target_model: ``str``\n",
    "            The path at which the target model will be saved\n",
    "        \"\"\"\n",
    "\n",
    "        if name is None:\n",
    "            path_model = path\n",
    "        else:\n",
    "            path_model = os.path.join(path, name)\n",
    "        path_target_model = \"{}_target\".format(path_model)\n",
    "        return path_model, path_target_model\n",
    "\n",
    "    def save_network(self, path, name=None, ext=\"h5\"):\n",
    "        \"\"\"\n",
    "        save the neural networks.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: ``str``\n",
    "            The path at which the models need to be saved\n",
    "        name: ``str``\n",
    "            The name given to this model\n",
    "\n",
    "        ext: ``str``\n",
    "            The file extension (by default h5)\n",
    "        \"\"\"\n",
    "        # Saves model at specified path as h5 file\n",
    "        # nothing has changed\n",
    "        path_model, path_target_model = self.get_path_model(path, name)\n",
    "        self._model.save('{}.{}'.format(path_model, ext))\n",
    "        self._target_model.save('{}.{}'.format(path_target_model, ext))\n",
    "\n",
    "    def load_network(self, path, name=None, ext=\"h5\"):\n",
    "        \"\"\"\n",
    "        Load the neural networks.\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: ``str``\n",
    "            The path at which the models need to be saved\n",
    "        name: ``str``\n",
    "            The name given to this model\n",
    "\n",
    "        ext: ``str``\n",
    "            The file extension (by default h5)\n",
    "        \"\"\"\n",
    "        path_model, path_target_model = self.get_path_model(path, name)\n",
    "        # fix for issue https://github.com/keras-team/keras/issues/7440\n",
    "        self.construct_q_network()\n",
    "\n",
    "        self._model.load_weights('{}.{}'.format(path_model, ext))\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            self._target_model.load_weights('{}.{}'.format(path_target_model, ext))\n",
    "        if self.verbose:\n",
    "            print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self, tau=None):\n",
    "        \"\"\"\n",
    "        update the target model with the parameters given in the :attr:`BaseDeepSARSA._training_param`.\n",
    "        \"\"\"\n",
    "        if tau is None:\n",
    "            tau = self._training_param.tau\n",
    "        tau_inv = 1.0 - tau\n",
    "\n",
    "        target_params = self._target_model.trainable_variables\n",
    "        source_params = self._model.trainable_variables\n",
    "        for src, dest in zip(source_params, target_params):\n",
    "            # Polyak averaging\n",
    "            var_update = src.value() * tau\n",
    "            var_persist = dest.value() * tau_inv\n",
    "            dest.assign(var_update + var_persist)\n",
    "\n",
    "    def save_tensorboard(self, current_step):\n",
    "        \"\"\"function used to save other information to tensorboard\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f5bbb",
   "metadata": {},
   "source": [
    "## Saving of trained neural networks\n",
    "This class provides an easy way to save and restore, as json, the shape of your neural networks (number of layers, non linearities, size of each layers etc.)\n",
    "        \n",
    "#### Attributes\n",
    "\n",
    "nn_class: :class:`l2rpn_baselines.BaseDeepSARSA`\n",
    "        The neural network class that will be created with each call of :func:`l2rpn_baselines.make_nn`\n",
    "\n",
    "observation_size: ``int``\n",
    "        The size of the observation space.\n",
    "\n",
    "action_size: ``int``\n",
    "        The size of the action space.\n",
    "\n",
    "sizes: ``list``\n",
    "        A list of integer, each will represent the number of hidden units. The number of hidden layer is given by\n",
    "        the size / length of this list.\n",
    "\n",
    "activs: ``list``\n",
    "        List of activation functions (given as string). It should have the same length as the :attr:`NNParam.sizes`.\n",
    "        This function should be name of keras activation function.\n",
    "\n",
    "list_attr_obs: ``list``\n",
    "        List of the attributes that will be used from the observation and concatenated to be fed to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2612e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNParam(object):\n",
    "\n",
    "    _int_attr = [\"action_size\", \"observation_size\"]\n",
    "    _float_attr = []\n",
    "    _str_attr = []\n",
    "    _list_float = []\n",
    "    _list_str = [\"activs\", \"list_attr_obs\"]\n",
    "    _list_int = [\"sizes\"]\n",
    "    nn_class = BaseDeepSARSA\n",
    "\n",
    "    def __init__(self,\n",
    "                 action_size,\n",
    "                 observation_size,\n",
    "                 sizes,\n",
    "                 activs,\n",
    "                 list_attr_obs,\n",
    "                 ):\n",
    "        self.observation_size = observation_size\n",
    "        self.action_size = action_size\n",
    "        self.sizes = [int(el) for el in sizes]\n",
    "        self.activs = [str(el) for el in activs]\n",
    "        if len(self.sizes) != len(self.activs):\n",
    "            raise RuntimeError(\"\\\"sizes\\\" and \\\"activs\\\" lists have not the same size. It's not clear how many layers \"\n",
    "                               \"you want your neural network to have.\")\n",
    "        self.list_attr_obs = [str(el) for el in list_attr_obs]\n",
    "\n",
    "    @classmethod\n",
    "    def get_path_model(cls, path, name=None):\n",
    "        \"\"\"get the path at which the model will be saved\"\"\"\n",
    "        return cls.nn_class.get_path_model(path, name=name)\n",
    "\n",
    "    def make_nn(self, training_param):\n",
    "        \"\"\"build the appropriate BaseDeepSARSA\"\"\"\n",
    "        res = self.nn_class(self, training_param)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def get_obs_size(env, list_attr_name):\n",
    "        \"\"\"get the size of the flatten observation\"\"\"\n",
    "        res = 0\n",
    "        for obs_attr_name in list_attr_name:\n",
    "            beg_, end_, dtype_ = env.observation_space.get_indx_extract(obs_attr_name)\n",
    "            res += end_ - beg_  # no \"+1\" needed because \"end_\" is exclude by python convention\n",
    "        return res\n",
    "\n",
    "    def get_obs_attr(self):\n",
    "        \"\"\"get the names of the observation attributes that will be extracted \"\"\"\n",
    "        return self.list_attr_obs\n",
    "\n",
    "    # utilitaries, do not change\n",
    "    def to_dict(self):\n",
    "        \"\"\"convert this instance to a dictionnary\"\"\"\n",
    "        # TODO copy and paste from TrainingParam\n",
    "        res = {}\n",
    "        for attr_nm in self._int_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = int(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "        for attr_nm in self._float_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = float(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "        for attr_nm in self._str_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = str(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "\n",
    "        for attr_nm in self._list_float:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            res[attr_nm] = self._convert_list_to_json(tmp, float)\n",
    "        for attr_nm in self._list_int:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            res[attr_nm] = self._convert_list_to_json(tmp, int)\n",
    "        for attr_nm in self._list_str:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            res[attr_nm] = self._convert_list_to_json(tmp, str)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _convert_list_to_json(cls, obj, type_):\n",
    "        if isinstance(obj, type_):\n",
    "            res = obj\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            if len(obj.shape) == 1:\n",
    "                res = [type_(el) for el in obj]\n",
    "            else:\n",
    "                res = [cls._convert_list_to_json(el, type_) for el in obj]\n",
    "        elif isinstance(obj, Iterable):\n",
    "            res = [cls._convert_list_to_json(el, type_) for el in obj]\n",
    "        else:\n",
    "            res = type_(obj)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _attr_from_json(cls, json, type_):\n",
    "        if isinstance(json, type_):\n",
    "            res = json\n",
    "        elif isinstance(json, list):\n",
    "            res = [cls._convert_list_to_json(obj=el, type_=type_) for el in json]\n",
    "        else:\n",
    "            res = type_(json)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, tmp):\n",
    "        \"\"\"load from a dictionnary\"\"\"\n",
    "        # TODO copy and paste from TrainingParam (more or less)\n",
    "        cls_as_dict = {}\n",
    "        for attr_nm in cls._int_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    cls_as_dict[attr_nm] = int(tmp_)\n",
    "                else:\n",
    "                    cls_as_dict[attr_nm] = None\n",
    "\n",
    "        for attr_nm in cls._float_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    cls_as_dict[attr_nm] = float(tmp_)\n",
    "                else:\n",
    "                    cls_as_dict[attr_nm] = None\n",
    "\n",
    "        for attr_nm in cls._str_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    cls_as_dict[attr_nm] = str(tmp_)\n",
    "                else:\n",
    "                    cls_as_dict[attr_nm] = None\n",
    "\n",
    "        for attr_nm in cls._list_float:\n",
    "            if attr_nm in tmp:\n",
    "                cls_as_dict[attr_nm] = cls._attr_from_json(tmp[attr_nm], float)\n",
    "        for attr_nm in cls._list_int:\n",
    "            if attr_nm in tmp:\n",
    "                cls_as_dict[attr_nm] = cls._attr_from_json(tmp[attr_nm], int)\n",
    "        for attr_nm in cls._list_str:\n",
    "            if attr_nm in tmp:\n",
    "                cls_as_dict[attr_nm] = cls._attr_from_json(tmp[attr_nm], str)\n",
    "\n",
    "        res = cls(**cls_as_dict)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_path):\n",
    "        \"\"\"load from a json file\"\"\"\n",
    "        # TODO copy and paste from TrainingParam\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(\"No path are located at \\\"{}\\\"\".format(json_path))\n",
    "        with open(json_path, \"r\") as f:\n",
    "            dict_ = json.load(f)\n",
    "        return cls.from_dict(dict_)\n",
    "\n",
    "    def save_as_json(self, path, name=None):\n",
    "        \"\"\"save as a json file\"\"\"\n",
    "        # TODO copy and paste from TrainingParam\n",
    "        res = self.to_dict()\n",
    "        if name is None:\n",
    "            name = \"neural_net_parameters.json\"\n",
    "        if not os.path.exists(path):\n",
    "            raise RuntimeError(\"Directory \\\"{}\\\" not found to save the NN parameters\".format(path))\n",
    "        if not os.path.isdir(path):\n",
    "            raise NotADirectoryError(\"\\\"{}\\\" should be a directory\".format(path))\n",
    "        path_out = os.path.join(path, name)\n",
    "        with open(path_out, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(res, fp=f, indent=4, sort_keys=True)\n",
    "\n",
    "    def center_reduce(self, env):\n",
    "        \"\"\"currently not implemented for this class, \"coming soon\" as we might say\"\"\"\n",
    "        # TODO see TestLeapNet for this feature\n",
    "        self._center_reduce_vect(env.get_obs(), \"x\")\n",
    "\n",
    "    def _get_adds_mults_from_name(self, obs, attr_nm):\n",
    "        if attr_nm in [\"prod_p\"]:\n",
    "            add_tmp = np.array([-0.5 * (pmax + pmin) for pmin, pmax in zip(obs.gen_pmin, obs.gen_pmax)])\n",
    "            mult_tmp = np.array([1. / max((pmax - pmin), 0.) for pmin, pmax in zip(obs.gen_pmin, obs.gen_pmax)])\n",
    "        elif attr_nm in [\"prod_q\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1. / max(abs(val), 1.0) for val in obs.prod_q])\n",
    "        elif attr_nm in [\"load_p\", \"load_q\"]:\n",
    "            add_tmp = np.array([-val for val in getattr(obs, attr_nm)])\n",
    "            mult_tmp = 0.5\n",
    "        elif attr_nm in [\"load_v\", \"prod_v\", \"v_or\", \"v_ex\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1. / val for val in getattr(obs, attr_nm)])\n",
    "        elif attr_nm == \"hour_of_day\":\n",
    "            add_tmp = -12.\n",
    "            mult_tmp = 1.0 / 12\n",
    "        elif attr_nm == \"minute_of_hour\":\n",
    "            add_tmp = -30.\n",
    "            mult_tmp = 1.0 / 30\n",
    "        elif attr_nm == \"day_of_week\":\n",
    "            add_tmp = -4.\n",
    "            mult_tmp = 1.0 / 4\n",
    "        elif attr_nm == \"day\":\n",
    "            add_tmp = -15.\n",
    "            mult_tmp = 1.0 / 15.\n",
    "        elif attr_nm in [\"target_dispatch\", \"actual_dispatch\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1. / (pmax - pmin) for pmin, pmax in zip(obs.gen_pmin, obs.gen_pmax)])\n",
    "        elif attr_nm in [\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"q_or\", \"q_ex\"]:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = np.array([1.0 / max(val, 1.0) for val in getattr(obs, attr_nm)])\n",
    "        else:\n",
    "            add_tmp = 0.\n",
    "            mult_tmp = 1.0\n",
    "        return add_tmp, mult_tmp\n",
    "\n",
    "    def _center_reduce_vect(self, obs, nn_part):\n",
    "        \"\"\"\n",
    "        compute the xxxx_adds and xxxx_mults for one part of the neural network called nn_part,\n",
    "        depending on what attribute of the observation is extracted\n",
    "        \"\"\"\n",
    "        if not isinstance(obs, grid2op.Observation.BaseObservation):\n",
    "            # in multi processing i receive a set of observation there so i might need\n",
    "            # to extract only the first one\n",
    "            obs = obs[0]\n",
    "\n",
    "        li_attr_obs = getattr(self, \"list_attr_obs_{}\".format(nn_part))\n",
    "        adds = []\n",
    "        mults = []\n",
    "        for attr_nm in li_attr_obs:\n",
    "            add_tmp, mult_tmp = self._get_adds_mults_from_name(obs, attr_nm)\n",
    "            mults.append(mult_tmp)\n",
    "            adds.append(add_tmp)\n",
    "        setattr(self, \"{}_adds\".format(nn_part), adds)\n",
    "        setattr(self, \"{}_mults\".format(nn_part), mults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329fd74f",
   "metadata": {},
   "source": [
    "## To save training parameters of the model\n",
    "\n",
    "A class to store the training parameters of the models.\n",
    "\t\n",
    "        \n",
    "#### Attributes\n",
    "\n",
    "buffer_size: ``int``\n",
    "        Size of the replay buffer\n",
    "\n",
    "minibatch_size: ``int``\n",
    "        Size of the training minibatch\n",
    "update_freq: ``int``\n",
    "        Frequency at which the model is trained. Model is trained once every `update_freq` steps using `minibatch_size`\n",
    "        from an experience replay buffer.\n",
    "\n",
    "final_epsilon: ``float``\n",
    "        value for the final epsilon (for the e-greedy)\n",
    "initial_epsilon: ``float``\n",
    "        value for the initial epsilon (for the e-greedy)\n",
    "step_for_final_epsilon: ``int``\n",
    "        number of step at which the final epsilon (for the epsilon greedy exploration) will be reached\n",
    "\n",
    "min_observation: ``int``\n",
    "        number of observations before starting to train the neural nets. Before this number of iterations, the agent\n",
    "        will simply interact with the environment.\n",
    "\n",
    "lr: ``float``\n",
    "        The initial learning rate\n",
    "\n",
    "lr_decay_steps: ``int``\n",
    "        The learning rate decay step\n",
    "\n",
    "lr_decay_rate: ``float``\n",
    "        The learning rate decay rate\n",
    "\n",
    "num_frames: ``int``\n",
    "        Currently not used\n",
    "\n",
    "discount_factor: ``float``\n",
    "        The discount factor (a high discount factor is in favor of longer episode, a small one not really). This is\n",
    "        often called \"gamma\" in some RL paper. It's the gamma in: \"RL wants to minize the sum of the dicounted reward,\n",
    "        which are sum_{t >= t_0} \\gamma^{t - t_0} r_t\n",
    "\n",
    "tau: ``float``\n",
    "        Update the target model. Target model is updated according to\n",
    "        $target_model_weights[i] = self.training_param.tau * model_weights[i] + (1 - self.training_param.tau) * \\\n",
    "                                              target_model_weights[i]$\n",
    "\n",
    "min_iter: ``int``\n",
    "        It is possible in the training schedule to limit the number of time steps an episode can last. This is mainly\n",
    "        useful at beginning of training, to not get in a state where the grid has been modified so much the agent\n",
    "        will never get into a state resembling this one ever again). Stopping the episode before this happens can\n",
    "        help the learning.\n",
    "\n",
    "max_iter: ``int``\n",
    "        Just like \"min_iter\" but instead of being the minimum number of iteration, it's the maximum.\n",
    "\n",
    "update_nb_iter: ``int``\n",
    "        If max_iter_fun is the default one, this numer give the number of time we need to succeed a scenario before\n",
    "        having to increase the maximum number of timestep allowed\n",
    "\n",
    "step_increase_nb_iter: ``int`` or  ``None``\n",
    "        Of how many timestep we increase the maximum number of timesteps allowed per episode. Set it to O to deactivate\n",
    "        this.\n",
    "\n",
    "max_iter_fun: ``function``\n",
    "        A function that return the maximum number of steps an episode can count as for the current epoch. For example\n",
    "        it can be `max_iter_fun = lambda epoch_num : np.sqrt(50 * epoch_num)`\n",
    "        [default lambda x: x / self.update_nb_iter]\n",
    "\n",
    "oversampling_rate: ``float`` or ``None``\n",
    "        Set it to None to deactivate the oversampling of hard scenarios. Otherwise, this oversampling is done\n",
    "        with something like `proba = 1. / (time_step_lived**oversampling_rate + 1)` where `proba` is the probability\n",
    "        to be selected at the next call to \"reset\" and `time_step_lived` is the number of time steps\n",
    "\n",
    "random_sample_datetime_start: ``int`` or ``None``\n",
    "        If ``None`` during training the chronics will always start at the datetime the chronics start.\n",
    "        Otherwise, the training scheme will skip a number of time steps between 0 and  `random_sample_datetime_start`\n",
    "        when loading the next chronics. This is particularly useful when you want your agent to learn to operate\n",
    "        the grid regardless of the hour of day or day of the week.\n",
    "\n",
    "update_tensorboard_freq: ``int``\n",
    "        Frequency at which tensorboard is refresh (tensorboard summaries are saved every update_tensorboard_freq\n",
    "        steps)\n",
    "\n",
    "save_model_each: ``int``\n",
    "        Frequency at which the model is saved (it is saved every \"save_model_each\" steps)\n",
    "\n",
    "max_global_norm_grad: ``float``\n",
    "        Maximum gradient norm allowed (can make the training more stable) default to None if deactivated.\n",
    "        Not all baselines are compatible.\n",
    "\n",
    "max_value_grad: ``float``\n",
    "        Maximum value the gradient can take. Assign it to ``None`` to deactivate it. This can make the training\n",
    "        more stable in some cases, but can slow down the training process too. Not all baselines are compatible.\n",
    "\n",
    "max_loss: ``float``\n",
    "        Clip the value of the loss function. Set it to ``None`` to deactivate it. Again, this can make the training\n",
    "        more stable but possibly slower. Not all baselines are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88dd7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingParam(object):\n",
    "    \n",
    "    _tol_float_equal = float(1e-8)\n",
    "\n",
    "    _int_attr = [\"buffer_size\", \"minibatch_size\", \"step_for_final_epsilon\",\n",
    "                 \"min_observation\", \"last_step\", \"num_frames\", \"update_freq\",\n",
    "                 \"min_iter\", \"max_iter\", \"update_tensorboard_freq\", \"save_model_each\", \"_update_nb_iter\",\n",
    "                 \"step_increase_nb_iter\", \"min_observe\", \"sample_one_random_action_begin\"]\n",
    "    _float_attr = [\"_final_epsilon\", \"_initial_epsilon\", \"lr\", \"lr_decay_steps\", \"lr_decay_rate\",\n",
    "                   \"discount_factor\", \"tau\", \"oversampling_rate\",\n",
    "                   \"max_global_norm_grad\", \"max_value_grad\", \"max_loss\"]\n",
    "\n",
    "    def __init__(self,\n",
    "                 buffer_size=40000,\n",
    "                 minibatch_size=64,\n",
    "                 step_for_final_epsilon=100000,  # step at which min_espilon is obtain\n",
    "                 min_observation=5000,  # 5000\n",
    "                 final_epsilon=1./(7*288.),  # have on average 1 random action per week of approx 7*288 time steps\n",
    "                 initial_epsilon=0.4,\n",
    "                 lr=1e-4,\n",
    "                 lr_decay_steps=10000,\n",
    "                 lr_decay_rate=0.999,\n",
    "                 num_frames=1,\n",
    "                 discount_factor=0.99,\n",
    "                 tau=0.01,\n",
    "                 update_freq=256,\n",
    "                 min_iter=50,\n",
    "                 max_iter=8064,  # 1 month\n",
    "                 update_nb_iter=10,\n",
    "                 step_increase_nb_iter=0,  # by default no oversampling / under sampling based on difficulty\n",
    "                 update_tensorboard_freq=1000,  # update tensorboard every \"update_tensorboard_freq\" steps\n",
    "                 save_model_each=10000,  # save the model every \"update_tensorboard_freq\" steps\n",
    "                 random_sample_datetime_start=None,\n",
    "                 oversampling_rate=None,\n",
    "                 max_global_norm_grad=None,\n",
    "                 max_value_grad=None,\n",
    "                 max_loss=None,\n",
    "\n",
    "                 # observer: let the neural network \"observe\" for a given amount of time\n",
    "                 # all actions are replaced by a do nothing\n",
    "                 min_observe=None,\n",
    "\n",
    "                 # i do a random action at the beginning of an episode until a certain number of step\n",
    "                 # is made\n",
    "                 # it's recommended to have \"min_observe\" to be larger that this (this is an int)\n",
    "                 sample_one_random_action_begin=None,\n",
    "                 ):\n",
    "\n",
    "        self.random_sample_datetime_start = random_sample_datetime_start\n",
    "\n",
    "        self.buffer_size = int(buffer_size)\n",
    "        self.minibatch_size = int(minibatch_size)\n",
    "        self.min_observation = int(min_observation)\n",
    "        self._final_epsilon = float(final_epsilon)  # have on average 1 random action per day of approx 288 timesteps at the end (never kill completely the exploration)\n",
    "        self._initial_epsilon = float(initial_epsilon)\n",
    "        self.step_for_final_epsilon = float(step_for_final_epsilon)\n",
    "        self.lr = float(lr)\n",
    "        self.lr_decay_steps = float(lr_decay_steps)\n",
    "        self.lr_decay_rate = float(lr_decay_rate)\n",
    "\n",
    "        # gradient clipping (if supported)\n",
    "        self.max_global_norm_grad = max_global_norm_grad\n",
    "        self.max_value_grad = max_value_grad\n",
    "        self.max_loss = max_loss\n",
    "\n",
    "        # observer\n",
    "        self.min_observe = min_observe\n",
    "        self.sample_one_random_action_begin = sample_one_random_action_begin\n",
    "\n",
    "        self.last_step = int(0)\n",
    "        self.num_frames = int(num_frames)\n",
    "        self.discount_factor = float(discount_factor)\n",
    "        self.tau = float(tau)\n",
    "        self.update_freq = int(update_freq)\n",
    "        self.min_iter = int(min_iter)\n",
    "        self.max_iter = int(max_iter)\n",
    "        self._1_update_nb_iter = None\n",
    "        self._update_nb_iter = int(update_nb_iter)\n",
    "        if step_increase_nb_iter is None:\n",
    "            # 0 and None have the same effect: it disable the feature\n",
    "            step_increase_nb_iter = 0\n",
    "        self.step_increase_nb_iter = step_increase_nb_iter\n",
    "\n",
    "        if oversampling_rate is not None:\n",
    "            self.oversampling_rate = float(oversampling_rate)\n",
    "        else:\n",
    "            self.oversampling_rate = None\n",
    "\n",
    "        self.update_tensorboard_freq = update_tensorboard_freq\n",
    "        self.save_model_each = save_model_each\n",
    "        self.max_iter_fun = self.default_max_iter_fun\n",
    "        self._compute_exp_facto()\n",
    "\n",
    "    @property\n",
    "    def final_epsilon(self):\n",
    "        return self._final_epsilon\n",
    "\n",
    "    @final_epsilon.setter\n",
    "    def final_epsilon(self, final_epsilon):\n",
    "        self._final_epsilon = final_epsilon\n",
    "        self._compute_exp_facto()\n",
    "\n",
    "    @property\n",
    "    def initial_epsilon(self):\n",
    "        return self._initial_epsilon\n",
    "\n",
    "    @initial_epsilon.setter\n",
    "    def initial_epsilon(self, initial_epsilon):\n",
    "        self._initial_epsilon = initial_epsilon\n",
    "        self._compute_exp_facto()\n",
    "\n",
    "    @property\n",
    "    def update_nb_iter(self):\n",
    "        return self._update_nb_iter\n",
    "\n",
    "    @update_nb_iter.setter\n",
    "    def update_nb_iter(self, update_nb_iter):\n",
    "        self._update_nb_iter = update_nb_iter\n",
    "        if self._update_nb_iter is not None and self._update_nb_iter > 0:\n",
    "            self._1_update_nb_iter = 1.0 / self._update_nb_iter\n",
    "        else:\n",
    "            self._1_update_nb_iter = 1.0\n",
    "\n",
    "    def _compute_exp_facto(self):\n",
    "        if self.final_epsilon is not None and self.initial_epsilon is not None and self.final_epsilon > 0:\n",
    "            self._exp_facto = np.log(self.initial_epsilon/self.final_epsilon)\n",
    "        else:\n",
    "            # TODO\n",
    "            self._exp_facto = 1\n",
    "\n",
    "    def default_max_iter_fun(self, nb_success):\n",
    "        \"\"\"the default max iteration function used\"\"\"\n",
    "        return self.step_increase_nb_iter * int(nb_success * self._1_update_nb_iter)\n",
    "\n",
    "    def tell_step(self, current_step):\n",
    "        \"\"\"tell this instance the number of training steps that have been made\"\"\"\n",
    "        self.last_step = current_step\n",
    "\n",
    "    def get_next_epsilon(self, current_step):\n",
    "        \"\"\"get the next epsilon for the e greedy exploration\"\"\"\n",
    "        self.tell_step(current_step)\n",
    "        if self.step_for_final_epsilon is None or self.initial_epsilon is None \\\n",
    "                or self._exp_facto is None or self.final_epsilon is None:\n",
    "            res = 0.\n",
    "        else:\n",
    "            if current_step > self.step_for_final_epsilon:\n",
    "                res = self.final_epsilon\n",
    "            else:\n",
    "                # exponential decrease\n",
    "                res = self.initial_epsilon * np.exp(- (current_step / self.step_for_final_epsilon) * self._exp_facto )\n",
    "        return res\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"serialize this instance to a dictionnary.\"\"\"\n",
    "        res = {}\n",
    "        for attr_nm in self._int_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = int(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "        for attr_nm in self._float_attr:\n",
    "            tmp = getattr(self, attr_nm)\n",
    "            if tmp is not None:\n",
    "                res[attr_nm] = float(tmp)\n",
    "            else:\n",
    "                res[attr_nm] = None\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(tmp):\n",
    "        \"\"\"initialize this instance from a dictionary\"\"\"\n",
    "        if not isinstance(tmp, dict):\n",
    "            raise RuntimeError(\"TrainingParam from dict must be called with a dictionary, and not {}\".format(tmp))\n",
    "        res = TrainingParam()\n",
    "        for attr_nm in TrainingParam._int_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    setattr(res, attr_nm, int(tmp_))\n",
    "                else:\n",
    "                    setattr(res, attr_nm, None)\n",
    "\n",
    "        for attr_nm in TrainingParam._float_attr:\n",
    "            if attr_nm in tmp:\n",
    "                tmp_ = tmp[attr_nm]\n",
    "                if tmp_ is not None:\n",
    "                    setattr(res, attr_nm, float(tmp_))\n",
    "                else:\n",
    "                    setattr(res, attr_nm, None)\n",
    "        res.update_nb_iter = res._update_nb_iter\n",
    "        res.initial_epsilon = res._initial_epsilon\n",
    "        res._compute_exp_facto()\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(json_path):\n",
    "        \"\"\"initialize this instance from a json\"\"\"\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(\"No path are located at \\\"{}\\\"\".format(json_path))\n",
    "        with open(json_path, \"r\") as f:\n",
    "            dict_ = json.load(f)\n",
    "        return TrainingParam.from_dict(dict_)\n",
    "\n",
    "    def save_as_json(self, path, name=None):\n",
    "        \"\"\"save this instance as a json\"\"\"\n",
    "        res = self.to_dict()\n",
    "        if name is None:\n",
    "            name = \"training_parameters.json\"\n",
    "        if not os.path.exists(path):\n",
    "            raise RuntimeError(\"Directory \\\"{}\\\" not found to save the training parameters\".format(path))\n",
    "        if not os.path.isdir(path):\n",
    "            raise NotADirectoryError(\"\\\"{}\\\" should be a directory\".format(path))\n",
    "        path_out = os.path.join(path, name)\n",
    "        with open(path_out, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(res, fp=f, indent=4, sort_keys=True)\n",
    "\n",
    "    def do_train(self):\n",
    "        \"\"\"return whether or not i should train the model at this time step\"\"\"\n",
    "        return self.last_step % self.update_freq == 0\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        res = True\n",
    "        for el in self._int_attr:\n",
    "            me_ = getattr(self, el)\n",
    "            oth_ = getattr(other, el)\n",
    "            if me_ is None and oth_ is not None:\n",
    "                res = False\n",
    "                break\n",
    "            if oth_ is None and me_ is not None:\n",
    "                res = False\n",
    "                break\n",
    "            if me_ is None and oth_ is None:\n",
    "                continue\n",
    "            if int(me_) != int(oth_):\n",
    "                res = False\n",
    "                break\n",
    "        if res:\n",
    "            for el in self._float_attr:\n",
    "                me_ = getattr(self, el)\n",
    "                oth_ = getattr(other, el)\n",
    "                if me_ is None and oth_ is not None:\n",
    "                    res = False\n",
    "                    break\n",
    "                if oth_ is None and me_ is not None:\n",
    "                    res = False\n",
    "                    break\n",
    "                if me_ is None and oth_ is None:\n",
    "                    continue\n",
    "                if abs(float(me_) - float(oth_)) > self._tol_float_equal:\n",
    "                    res = False\n",
    "                    break\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab1f53",
   "metadata": {},
   "source": [
    "## Create neural network using abstract class created above\n",
    "Constructs the desired deep q learning network\n",
    "        \n",
    "#### Attributes\n",
    "\n",
    "schedule_lr_model:\n",
    "        The schedule for the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca815187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQ_NN(BaseDeepSARSA):\n",
    "\n",
    "    def __init__(self,\n",
    "                 nn_params,\n",
    "                 training_param=None):\n",
    "        if not _CAN_USE_TENSORFLOW:\n",
    "            raise RuntimeError(\"Cannot import tensorflow, this function cannot be used.\")\n",
    "        \n",
    "        if training_param is None:\n",
    "            training_param = TrainingParam()\n",
    "        BaseDeepSARSA.__init__(self,\n",
    "                           nn_params,\n",
    "                           training_param)\n",
    "        self.schedule_lr_model = None\n",
    "        self.construct_q_network()\n",
    "\n",
    "    def construct_q_network(self):\n",
    "        \"\"\"\n",
    "        This function will make 2 identical models, one will serve as a target model, the other one will be trained\n",
    "        regurlarly.\n",
    "        \"\"\"\n",
    "        self._model = Sequential()\n",
    "        input_layer = Input(shape=(self._nn_archi.observation_size,),\n",
    "                            name=\"state\")\n",
    "        lay = input_layer\n",
    "        for lay_num, (size, act) in enumerate(zip(self._nn_archi.sizes, self._nn_archi.activs)):\n",
    "            lay = Dense(size, name=\"layer_{}\".format(lay_num))(lay)  # put at self.action_size\n",
    "            lay = Activation(act)(lay)\n",
    "\n",
    "        output = Dense(self._action_size, name=\"output\")(lay)\n",
    "\n",
    "        self._model = Model(inputs=[input_layer], outputs=[output])\n",
    "        self._schedule_lr_model, self._optimizer_model = self.make_optimiser()\n",
    "        self._model.compile(loss='mse', optimizer=self._optimizer_model)\n",
    "\n",
    "        self._target_model = Model(inputs=[input_layer], outputs=[output])\n",
    "        self._target_model.set_weights(self._model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68582acd",
   "metadata": {},
   "source": [
    "## Define parameters for deep neural nets\n",
    "This defined the specific parameters for the DeepQ network. \n",
    "Nothing really different compared to the base class except that :attr:`l2rpn_baselines.utils.NNParam.nn_class` (nn_class) is :class:`deepQ_NN.DeepQ_NN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abcf1e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQ_NNParam(NNParam):\n",
    "    _int_attr = copy.deepcopy(NNParam._int_attr)\n",
    "    _float_attr = copy.deepcopy(NNParam._float_attr)\n",
    "    _str_attr = copy.deepcopy(NNParam._str_attr)\n",
    "    _list_float = copy.deepcopy(NNParam._list_float)\n",
    "    _list_str = copy.deepcopy(NNParam._list_str)\n",
    "    _list_int = copy.deepcopy(NNParam._list_int)\n",
    "\n",
    "    nn_class = DeepQ_NN\n",
    "\n",
    "    def __init__(self,\n",
    "                 action_size,\n",
    "                 observation_size,  # TODO this might not be usefull\n",
    "                 sizes,\n",
    "                 activs,\n",
    "                 list_attr_obs\n",
    "                 ):\n",
    "        NNParam.__init__(self,\n",
    "                         action_size,\n",
    "                         observation_size,  # TODO this might not be usefull\n",
    "                         sizes,\n",
    "                         activs,\n",
    "                         list_attr_obs\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e239c0",
   "metadata": {},
   "source": [
    "## A Skeleton Class That Inherits DeepSARSAAgent Class\n",
    "A simple deep q learning algorithm. It does nothing different thant its base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f70c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQSimple(DeepSARSAAgent):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713eaf0",
   "metadata": {},
   "source": [
    "## A function to train the deep SARSA agent created above\n",
    "\n",
    "This function implements the \"training\" part of the balines \"DeepQSimple\".\n",
    "        \n",
    "#### Parameters\n",
    "\n",
    "env: :class:`grid2op.Environment`\n",
    "        Then environment on which you need to train your agent.\n",
    "\n",
    "name: ``str``\n",
    "        The name of your agent.\n",
    "\n",
    "iterations: ``int``\n",
    "        For how many iterations (steps) do you want to train your agent. NB these are not episode, these are steps.\n",
    "\n",
    "save_path: ``str``\n",
    "        Where do you want to save your baseline.\n",
    "\n",
    "load_path: ``str``\n",
    "        If you want to reload your baseline, specify the path where it is located. **NB** if a baseline is reloaded\n",
    "        some of the argument provided to this function will not be used.\n",
    "\n",
    "logs_dir: ``str``\n",
    "        Where to store the tensorboard generated logs during the training. ``None`` if you don't want to log them.\n",
    "\n",
    "training_param: :class:`l2rpn_baselines.utils.TrainingParam`\n",
    "        The parameters describing the way you will train your model.\n",
    "\n",
    "filter_action_fun: ``function``\n",
    "        A function to filter the action space. See\n",
    "        `IdToAct.filter_action <https://grid2op.readthedocs.io/en/latest/converter.html#grid2op.Converter.IdToAct.filter_action>`_\n",
    "        documentation.\n",
    "\n",
    "verbose: ``bool``\n",
    "        If you want something to be printed on the terminal (a better logging strategy will be put at some point)\n",
    "\n",
    "kwargs_converters: ``dict``\n",
    "        A dictionary containing the key-word arguments pass at this initialization of the\n",
    "        :class:`grid2op.Converter.IdToAct` that serves as \"Base\" for the Agent.\n",
    "\n",
    "kwargs_archi: ``dict``\n",
    "        Key word arguments used for making the :class:`DeepQ_NNParam` object that will be used to build the baseline.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "baseline: :class:`DeepQSimple`\n",
    "        The trained baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2972d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env,\n",
    "          name=DEFAULT_NAME,\n",
    "          iterations=1,\n",
    "          save_path=None,\n",
    "          load_path=None,\n",
    "          logs_dir=None,\n",
    "          training_param=None,\n",
    "          filter_action_fun=None,\n",
    "          kwargs_converters={},\n",
    "          kwargs_archi={},\n",
    "          verbose=True):\n",
    "    \n",
    "    import tensorflow as tf  # lazy import to save import time\n",
    "    # Limit gpu usage\n",
    "    try:\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        if len(physical_devices) > 0:\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except AttributeError:\n",
    "         # issue of https://stackoverflow.com/questions/59266150/attributeerror-module-tensorflow-core-api-v2-config-has-no-attribute-list-p\n",
    "        try:\n",
    "            physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "            if len(physical_devices) > 0:\n",
    "                tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        except Exception:\n",
    "            warnings.warn(_WARN_GPU_MEMORY)\n",
    "    except Exception:\n",
    "        warnings.warn(_WARN_GPU_MEMORY)\n",
    "\n",
    "    if training_param is None:\n",
    "        training_param = TrainingParam()\n",
    "\n",
    "    # compute the proper size for the converter\n",
    "    kwargs_archi[\"action_size\"] = DeepQSimple.get_action_size(env.action_space, filter_action_fun, kwargs_converters)\n",
    "\n",
    "    if load_path is not None:\n",
    "        path_model, path_target_model = DeepQ_NN.get_path_model(load_path, name)\n",
    "        if verbose:\n",
    "            print(\"INFO: Reloading a model, the architecture parameters will be ignored\")\n",
    "        nn_archi = DeepQ_NNParam.from_json(os.path.join(path_model, \"nn_architecture.json\"))\n",
    "    else:\n",
    "        nn_archi = DeepQ_NNParam(**kwargs_archi)\n",
    "\n",
    "    baseline = DeepQSimple(action_space=env.action_space,\n",
    "                           nn_archi=nn_archi,\n",
    "                           name=name,\n",
    "                           istraining=True,\n",
    "                           verbose=verbose,\n",
    "                           filter_action_fun=filter_action_fun,\n",
    "                            **kwargs_converters\n",
    "                            )\n",
    "\n",
    "    if load_path is not None:\n",
    "        if verbose:\n",
    "            print(\"INFO: Reloading a model, training parameters will be ignored\")\n",
    "        baseline.load(load_path)\n",
    "        training_param = baseline._training_param\n",
    "\n",
    "    baseline.train(env,\n",
    "                   iterations,\n",
    "                   save_path=save_path,\n",
    "                   logdir=logs_dir,\n",
    "                   training_param=training_param)\n",
    "    # as in our example (and in our explanation) we recommend to save the mode regurlarly in the \"train\" function\n",
    "    # it is not necessary to save it again here. But if you chose not to follow these advice, it is more than\n",
    "    # recommended to save the \"baseline\" at the end of this function with:\n",
    "    # baseline.save(path_save)\n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f723c8",
   "metadata": {},
   "source": [
    "## Method to evaluate the agent\n",
    "How to evaluate the performances of the trained :class:`DeepQSimple` agent.\n",
    "\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "env: :class:`grid2op.Environment`\n",
    "        The environment on which you evaluate your agent.\n",
    "\n",
    "name: ``str``\n",
    "        The name of the trained baseline\n",
    "\n",
    "load_path: ``str``\n",
    "        Path where the agent has been stored\n",
    "\n",
    "logs_path: ``str``\n",
    "        Where to write the results of the assessment\n",
    "\n",
    "nb_episode: ``str``\n",
    "        How many episodes to run during the assessment of the performances\n",
    "\n",
    "nb_process: ``int``\n",
    "        On how many process the assessment will be made. (setting this > 1 can lead to some speed ups but can be\n",
    "        unstable on some plaform)\n",
    "\n",
    "max_steps: ``int``\n",
    "        How many steps at maximum your agent will be assessed\n",
    "\n",
    "verbose: ``bool``\n",
    "        Currently un used\n",
    "\n",
    "save_gif: ``bool``\n",
    "        Whether or not you want to save, as a gif, the performance of your agent. It might cause memory issues (might\n",
    "        take a lot of ram) and drastically increase computation time.\n",
    "\n",
    "### Returns\n",
    "\n",
    "agent: :class:`l2rpn_baselines.utils.DeepSARSAAgent`\n",
    "        The loaded agent that has been evaluated thanks to the runner.\n",
    "\n",
    "res: ``list``\n",
    "        The results of the Runner on which the agent was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5157ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env,\n",
    "             name=DEFAULT_NAME,\n",
    "             load_path=None,\n",
    "             logs_path=DEFAULT_LOGS_DIR,\n",
    "             nb_episode=DEFAULT_NB_EPISODE,\n",
    "             nb_process=DEFAULT_NB_PROCESS,\n",
    "             max_steps=DEFAULT_MAX_STEPS,\n",
    "             verbose=False,\n",
    "             save_gif=False,\n",
    "             filter_action_fun=None):\n",
    "    \n",
    "\n",
    "    import tensorflow as tf  # lazy import to save import time\n",
    "    # Limit gpu usage\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(physical_devices):\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "    runner_params = env.get_params_for_runner()\n",
    "    runner_params[\"verbose\"] = verbose\n",
    "\n",
    "    if load_path is None:\n",
    "        raise RuntimeError(\"Cannot evaluate a model if there is nothing to be loaded.\")\n",
    "    path_model, path_target_model = DeepQ_NN.get_path_model(load_path, name)\n",
    "    nn_archi = DeepQ_NNParam.from_json(os.path.join(path_model, \"nn_architecture.json\"))\n",
    "\n",
    "    # Run\n",
    "    # Create agent\n",
    "    agent = DeepQSimple(action_space=env.action_space,\n",
    "                        name=name,\n",
    "                        store_action=nb_process == 1,\n",
    "                        nn_archi=nn_archi,\n",
    "                        observation_space=env.observation_space,\n",
    "                        filter_action_fun=filter_action_fun)\n",
    "\n",
    "    # Load weights from file\n",
    "    agent.load(load_path)\n",
    "\n",
    "    # Build runner\n",
    "    runner = Runner(**runner_params,\n",
    "                    agentClass=None,\n",
    "                    agentInstance=agent)\n",
    "\n",
    "    # Print model summary\n",
    "    stringlist = []\n",
    "    agent.deep_sarsa._model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    short_model_summary = \"\\n\".join(stringlist)\n",
    "    if verbose:\n",
    "        print(short_model_summary)\n",
    "\n",
    "    # Run\n",
    "    os.makedirs(logs_path, exist_ok=True)\n",
    "    res = runner.run(path_save=logs_path,\n",
    "                     nb_episode=nb_episode,\n",
    "                     nb_process=nb_process,\n",
    "                     max_iter=max_steps,\n",
    "                     pbar=verbose)\n",
    "\n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"Evaluation summary:\")\n",
    "        for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "            msg_tmp = \"chronics at: {}\".format(chron_name)\n",
    "            msg_tmp += \"\\ttotal score: {:.6f}\".format(cum_reward)\n",
    "            msg_tmp += \"\\ttime steps: {:.0f}/{:.0f}\".format(nb_time_step, max_ts)\n",
    "            print(msg_tmp)\n",
    "\n",
    "        if len(agent.dict_action):\n",
    "            # I output some of the actions played\n",
    "            print(\"The agent played {} different action\".format(len(agent.dict_action)))\n",
    "            for id_, (nb, act, types) in agent.dict_action.items():\n",
    "                print(\"Action with ID {} was played {} times\".format(id_, nb))\n",
    "                print(\"{}\".format(act))\n",
    "                print(\"-----------\")\n",
    "\n",
    "    if save_gif:\n",
    "        if verbose:\n",
    "            print(\"Saving the gif of the episodes\")\n",
    "        save_log_gif(logs_path, res)\n",
    "\n",
    "    return agent, res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cda061",
   "metadata": {},
   "source": [
    "## Below code snippet defines the network, trains it, and saves it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29810cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                         | 5119/500000 [04:05<8:23:11, 16.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tejus_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                        | 9998/500000 [08:08<7:39:44, 17.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[103  97  95  94  93  92  91  90  89  88]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[ 52  12  28 100  64 132   4 110  84   8]\n",
      "avg (accross all scenarios) number of timsteps played 17.32986111111111\n",
      "Time alive: [26.  6. 14. 50. 32. 66.  2. 55. 42.  4.]\n",
      "Avg time alive: 8.682291666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▉                                                                      | 19998/500000 [16:26<5:56:27, 22.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 183 169 170 171 172 173 174 175 176]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[72 52 40 12 72  4 64  8 83 23]\n",
      "avg (accross all scenarios) number of timsteps played 34.44965277777778\n",
      "Time alive: [36.  26.  20.   6.  36.   2.  32.   4.  41.5 11.5]\n",
      "Avg time alive: 17.2421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▍                                                                    | 30000/500000 [24:21<5:39:59, 23.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 207 214 213 212 211 210 209 208 206]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[213  76  21  17  72  15  27  17  46  16]\n",
      "avg (accross all scenarios) number of timsteps played 51.796875\n",
      "Time alive: [106.5  38.   10.5   8.5  36.    7.5  13.5   8.5  23.    8. ]\n",
      "Avg time alive: 25.91579861111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▊                                                                   | 39999/500000 [32:16<5:31:48, 23.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 331 322 323 324 325 326 327 328 329]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[213 315 264 292 240 216 260 320 408 158]\n",
      "avg (accross all scenarios) number of timsteps played 69.26909722222223\n",
      "Time alive: [106.5 157.5 132.  146.  120.  108.  130.  160.  204.   79. ]\n",
      "Avg time alive: 34.65190972222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▎                                                                 | 50000/500000 [40:07<5:43:22, 21.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 308 298 299 300 301 302 303 304 305]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[213 339  97 143  20  39  94 228 493 160]\n",
      "avg (accross all scenarios) number of timsteps played 85.83680555555556\n",
      "Time alive: [106.5 169.5  48.5  71.5  10.   19.5  47.  114.  246.5  80. ]\n",
      "Avg time alive: 42.935763888888886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████▊                                                                | 60000/500000 [48:05<5:34:50, 21.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 290 264 265 266 267 268 269 270 271]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[213 381 303 336 165  63  32 272  28   8]\n",
      "avg (accross all scenarios) number of timsteps played 103.95659722222223\n",
      "Time alive: [106.5 190.5 151.5 168.   82.5  31.5  16.  136.   14.    4. ]\n",
      "Avg time alive: 51.99565972222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████▏                                                              | 69998/500000 [56:11<5:12:21, 22.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 224 210 211 212 213 214 215 216 217]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[213   4  27  15  72  17  21  48  56  17]\n",
      "avg (accross all scenarios) number of timsteps played 121.32465277777777\n",
      "Time alive: [106.5   2.   13.5   7.5  36.    8.5  10.5  24.   28.    8.5]\n",
      "Avg time alive: 60.6796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████▎                                                           | 80000/500000 [1:04:08<5:09:31, 22.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[575 574 195 194 193 192 191 190 189 188]\n",
      "They have been chosen respectively\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "The number of timesteps played is\n",
      "[  0 277  72  33  25  24  25  23 108  11]\n",
      "avg (accross all scenarios) number of timsteps played 138.83506944444446\n",
      "Time alive: [  0.  138.5  36.   16.5  12.5  12.   12.5  11.5  54.    5.5]\n",
      "Avg time alive: 69.43489583333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████████▊                                                          | 89999/500000 [1:12:00<4:55:11, 23.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[31 16 32 30 29 28 27 26 25 24]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[684 609  24 313 176  93 153 539 123  49]\n",
      "avg (accross all scenarios) number of timsteps played 156.04340277777777\n",
      "Time alive: [228.         203.           8.         104.33333333  58.66666667\n",
      "  31.          51.         179.66666667  41.          16.33333333]\n",
      "Avg time alive: 74.64380787037038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████                                                        | 100000/500000 [1:19:55<4:48:18, 23.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[63 48 56 55 54 53 52 51 49 47]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[569 140 105  43  43 323 404 296 168 208]\n",
      "avg (accross all scenarios) number of timsteps played 172.95138888888889\n",
      "Time alive: [189.66666667  46.66666667  35.          14.33333333  14.33333333\n",
      " 107.66666667 134.66666667  98.66666667  56.          69.33333333]\n",
      "Avg time alive: 79.11979166666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████▍                                                      | 110000/500000 [1:27:48<4:43:48, 22.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[99 85 83 82 81 80 79 78 76 75]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[  16  169  644  706   96   76 1085  261   19  896]\n",
      "avg (accross all scenarios) number of timsteps played 189.10763888888889\n",
      "Time alive: [  5.33333333  56.33333333 214.66666667 235.33333333  32.\n",
      "  25.33333333 361.66666667  87.           6.33333333 298.66666667]\n",
      "Avg time alive: 83.77372685185186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████▊                                                     | 120000/500000 [1:35:37<4:32:29, 23.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[106  98  92  93  94  95  96  97  99  90]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[132 251 651  68 117 251  79 455  16 778]\n",
      "avg (accross all scenarios) number of timsteps played 208.17534722222223\n",
      "Time alive: [ 44.          83.66666667 217.          22.66666667  39.\n",
      "  83.66666667  26.33333333 151.66666667   5.33333333 259.33333333]\n",
      "Avg time alive: 89.67795138888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████████████▏                                                   | 130000/500000 [1:43:30<4:28:00, 23.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143  88 112 113 115 116 117 118 119 120]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[1565  131  283 1091 1132 1447  259  168  543  253]\n",
      "avg (accross all scenarios) number of timsteps played 225.62673611111111\n",
      "Time alive: [521.66666667  43.66666667  94.33333333 363.66666667 377.33333333\n",
      " 482.33333333  86.33333333  56.         181.          84.33333333]\n",
      "Avg time alive: 95.14351851851853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████▌                                                  | 140000/500000 [1:51:23<4:18:38, 23.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 149 137 138 139 140 141 142 144 145]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[1565  161  226   70  451   24  285  145  519  500]\n",
      "avg (accross all scenarios) number of timsteps played 242.86111111111111\n",
      "Time alive: [521.66666667  53.66666667  75.33333333  23.33333333 150.33333333\n",
      "   8.          95.          48.33333333 173.         166.66666667]\n",
      "Avg time alive: 100.40653935185186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████▉                                                 | 149998/500000 [1:59:14<4:13:31, 23.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 183 155 156 157 158 159 160 161 162]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[1565  685  278  413  149  436 1069  129 1093  137]\n",
      "avg (accross all scenarios) number of timsteps played 259.27256944444446\n",
      "Time alive: [521.66666667 228.33333333  92.66666667 137.66666667  49.66666667\n",
      " 145.33333333 356.33333333  43.         364.33333333  45.66666667]\n",
      "Avg time alive: 105.53935185185186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████▍                                               | 160000/500000 [2:07:08<4:05:45, 23.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 186 192 191 190 189 188 187 185 194]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[1565  196  265  688  113  232   15  188   31   48]\n",
      "avg (accross all scenarios) number of timsteps played 277.59722222222223\n",
      "Time alive: [521.66666667  65.33333333  88.33333333 229.33333333  37.66666667\n",
      "  77.33333333   5.          62.66666667  10.33333333  16.        ]\n",
      "Avg time alive: 109.12557870370371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████▊                                              | 169998/500000 [2:15:00<4:47:17, 19.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 204 211 210 209 208 207 206 205 203]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[300 223  48 249 453  59 331  36 223  46]\n",
      "avg (accross all scenarios) number of timsteps played 295.0208333333333\n",
      "Time alive: [100.          74.33333333  16.          83.         151.\n",
      "  19.66666667 110.33333333  12.          74.33333333  15.33333333]\n",
      "Avg time alive: 113.33564814814815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████████████▏                                            | 179999/500000 [2:22:50<3:57:50, 22.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 223 231 230 229 228 227 226 225 224]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 300  295  272  504   28  232  191  124  240 1941]\n",
      "avg (accross all scenarios) number of timsteps played 312.30381944444446\n",
      "Time alive: [100.          98.33333333  90.66666667 168.           9.33333333\n",
      "  77.33333333  63.66666667  41.33333333  80.         647.        ]\n",
      "Avg time alive: 117.33709490740742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████▌                                           | 190000/500000 [2:30:43<3:50:47, 22.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 329 320 321 322 323 324 325 326 327]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 300  672  209  674  476  372  284 1081  512  484]\n",
      "avg (accross all scenarios) number of timsteps played 329.7986111111111\n",
      "Time alive: [100.         224.          69.66666667 224.66666667 158.66666667\n",
      " 124.          94.66666667 360.33333333 170.66666667 161.33333333]\n",
      "Avg time alive: 121.6478587962963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████▉                                          | 199999/500000 [2:38:43<3:36:19, 23.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 283 285 286 148 288 289 290 291 292]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 300  824  626  493   28 1092  268  925  104  735]\n",
      "avg (accross all scenarios) number of timsteps played 347.22222222222223\n",
      "Time alive: [100.         274.66666667 208.66666667 164.33333333   9.33333333\n",
      " 364.          89.33333333 308.33333333  34.66666667 245.        ]\n",
      "Avg time alive: 122.40943287037038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████▍                                        | 210000/500000 [2:46:34<3:35:19, 22.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 262 264 265 266 267 268 269 270 271]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 300  893  584 1140  477  160   59  381  448  629]\n",
      "avg (accross all scenarios) number of timsteps played 364.49131944444446\n",
      "Time alive: [100.         297.66666667 194.66666667 380.         159.\n",
      "  53.33333333  19.66666667 127.         149.33333333 209.66666667]\n",
      "Avg time alive: 127.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████▊                                       | 219999/500000 [2:54:26<3:22:03, 23.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 240 242 243 244 245 246 247 248 249]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[ 300  931 1101 1095  306  269  259  220  527  422]\n",
      "avg (accross all scenarios) number of timsteps played 381.70659722222223\n",
      "Time alive: [100.         310.33333333 367.         365.         102.\n",
      "  89.66666667  86.33333333  73.33333333 175.66666667 140.66666667]\n",
      "Avg time alive: 132.16811342592595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████▏                                     | 229998/500000 [3:02:27<3:23:02, 22.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 225 211 212 213 214 215 216 217 218]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[300 240  48 319 526 276 732 136 385  52]\n",
      "avg (accross all scenarios) number of timsteps played 398.8923611111111\n",
      "Time alive: [100.          80.          16.         106.33333333 175.33333333\n",
      "  92.         244.          45.33333333 128.33333333  17.33333333]\n",
      "Avg time alive: 136.14988425925927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████▌                                    | 240000/500000 [3:10:31<3:12:56, 22.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 195 201 200 199 198 197 196 194 203]\n",
      "They have been chosen respectively\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "The number of timesteps played is\n",
      "[300 178 567 148 305  75 448 250  48  46]\n",
      "avg (accross all scenarios) number of timsteps played 415.77256944444446\n",
      "Time alive: [100.          59.33333333 189.          49.33333333 101.66666667\n",
      "  25.         149.33333333  83.33333333  16.          15.33333333]\n",
      "Avg time alive: 139.31539351851853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████▉                                   | 249998/500000 [3:18:35<3:19:21, 20.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[19 12 24 23 22 21 20 18 17 16]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 654 1235   49  300  506  499  296  306  414  870]\n",
      "avg (accross all scenarios) number of timsteps played 433.97569444444446\n",
      "Time alive: [163.5  308.75  12.25  75.   126.5  124.75  74.    76.5  103.5  217.5 ]\n",
      "Avg time alive: 142.28949652777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████████████████████▍                                 | 259998/500000 [3:26:39<2:57:04, 22.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[57 46 32 33 34 35 36 37 38 39]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[301 799 304 316 648 505 469 667 621 821]\n",
      "avg (accross all scenarios) number of timsteps played 451.05902777777777\n",
      "Time alive: [ 75.25 199.75  76.    79.   162.   126.25 117.25 166.75 155.25 205.25]\n",
      "Avg time alive: 145.11487268518516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████████████████████████████████████▊                                | 269999/500000 [3:34:40<2:49:35, 22.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[96 63 74 73 71 70 69 68 67 66]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[  95  684  813 1081  798  418  342  347  976  371]\n",
      "avg (accross all scenarios) number of timsteps played 467.94618055555554\n",
      "Time alive: [ 23.75 171.   203.25 270.25 199.5  104.5   85.5   86.75 244.    92.75]\n",
      "Avg time alive: 147.20905671296293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████████████████████████████████▏                              | 280000/500000 [3:42:46<2:43:29, 22.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[106  97  91  92  93  94  95  96  98  89]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 152 1720  577  784  136  217  423   95 1335  579]\n",
      "avg (accross all scenarios) number of timsteps played 485.3888888888889\n",
      "Time alive: [ 38.   430.   144.25 196.    34.    54.25 105.75  23.75 333.75 144.75]\n",
      "Avg time alive: 149.36993634259258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████▌                             | 289999/500000 [3:50:45<2:34:07, 22.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143  88 112 113 115 116 117 118 119 120]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[1584  155  308 1322 1332 1947  487  829  833  633]\n",
      "avg (accross all scenarios) number of timsteps played 503.34722222222223\n",
      "Time alive: [396.    38.75  77.   330.5  333.   486.75 121.75 207.25 208.25 158.25]\n",
      "Avg time alive: 152.16276041666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████▉                            | 299999/500000 [3:58:47<2:26:00, 22.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 159 134 135 136 137 138 139 140 141]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[1584 1868  262  246  224  246  201  678 1036  971]\n",
      "avg (accross all scenarios) number of timsteps played 520.2986111111111\n",
      "Time alive: [396.   467.    65.5   61.5   56.    61.5   50.25 169.5  259.   242.75]\n",
      "Avg time alive: 154.91869212962962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████▍                          | 309999/500000 [4:06:48<2:18:58, 22.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 172 159 160 161 162 163 164 165 166]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[1584  827 1868  421 1301  141  109  233  156  620]\n",
      "avg (accross all scenarios) number of timsteps played 537.8107638888889\n",
      "Time alive: [396.   206.75 467.   105.25 325.25  35.25  27.25  58.25  39.   155.  ]\n",
      "Avg time alive: 157.04123263888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████▊                         | 320000/500000 [4:14:49<2:17:25, 21.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 187 193 192 191 190 189 188 186 195]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[1584  352  907  470 1200  370  732  248  499  287]\n",
      "avg (accross all scenarios) number of timsteps played 555.046875\n",
      "Time alive: [396.    88.   226.75 117.5  300.    92.5  183.    62.   124.75  71.75]\n",
      "Avg time alive: 158.82175925925924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████████████████████████████████████████████▏                       | 329999/500000 [4:22:52<2:28:11, 19.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 225 223 222 221 220 219 218 217 216]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 540  404  479  646  584 1021  515  481  484  229]\n",
      "avg (accross all scenarios) number of timsteps played 571.6163194444445\n",
      "Time alive: [135.   101.   119.75 161.5  146.   255.25 128.75 120.25 121.    57.25]\n",
      "Avg time alive: 159.25332754629628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████████████████████████████▌                      | 340000/500000 [4:30:54<2:35:19, 17.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 327 329 330 331 332 333 334 335 336]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 540  536  833 1422  523  469  557  631  664 1190]\n",
      "avg (accross all scenarios) number of timsteps played 590.2638888888889\n",
      "Time alive: [135.   134.   208.25 355.5  130.75 117.25 139.25 157.75 166.   297.5 ]\n",
      "Avg time alive: 162.09939236111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████▉                     | 349999/500000 [4:38:55<1:50:20, 22.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 303 305 306 307 308 309 310 311 312]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 540  432 1251  647  136  624  733  630  890 1150]\n",
      "avg (accross all scenarios) number of timsteps played 607.234375\n",
      "Time alive: [135.   108.   312.75 161.75  34.   156.   183.25 157.5  222.5  287.5 ]\n",
      "Avg time alive: 163.70616319444446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|██████████████████████████████████████████████████▍                   | 359998/500000 [4:46:55<1:42:24, 22.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 280 282 283 284 285 286 144 288 289]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 540  574 1942  876 2158  630  510  797 1180  355]\n",
      "avg (accross all scenarios) number of timsteps played 624.6232638888889\n",
      "Time alive: [135.   143.5  485.5  219.   539.5  157.5  127.5  199.25 295.    88.75]\n",
      "Avg time alive: 165.34924768518516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████████████████████████████████████████████████▊                  | 369999/500000 [4:54:57<1:52:30, 19.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 267 255 256 257 258 259 260 261 262]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 540  188  585  864  411  635 1028  445  368 1105]\n",
      "avg (accross all scenarios) number of timsteps played 642.3402777777778\n",
      "Time alive: [135.    47.   146.25 216.   102.75 158.75 257.   111.25  92.   276.25]\n",
      "Avg time alive: 167.66565393518516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|█████████████████████████████████████████████████████▏                | 379999/500000 [5:03:03<1:29:58, 22.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 244 231 232 233 234 235 236 237 238]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[540 419 658 636 392 483 500 183 293 537]\n",
      "avg (accross all scenarios) number of timsteps played 659.5017361111111\n",
      "Time alive: [135.   104.75 164.5  159.    98.   120.75 125.    45.75  73.25 134.25]\n",
      "Avg time alive: 169.78819444444446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████▌               | 390000/500000 [5:11:10<1:22:07, 22.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 218 204 205 206 207 208 209 210 211]\n",
      "They have been chosen respectively\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "The number of timesteps played is\n",
      "[ 540  481  379  450  267 1117  597  582  767   60]\n",
      "avg (accross all scenarios) number of timsteps played 676.8107638888889\n",
      "Time alive: [135.   120.25  94.75 112.5   66.75 279.25 149.25 145.5  191.75  15.  ]\n",
      "Avg time alive: 171.76244212962962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████▉              | 399998/500000 [5:19:15<1:33:57, 17.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[ 8  1  2  3  4  5  6  7  9 10]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 633  520  723  581 1441 2317 1492 1930  710  314]\n",
      "avg (accross all scenarios) number of timsteps played 694.4270833333334\n",
      "Time alive: [126.6 104.  144.6 116.2 288.2 463.4 298.4 386.  142.   62.8]\n",
      "Avg time alive: 172.65416666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████████████████████████████▍            | 410000/500000 [5:27:17<1:07:12, 22.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[36 34 24 25 26 27 29 30 31 32]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 496 1172  266  165  785  513  629  504 1451  308]\n",
      "avg (accross all scenarios) number of timsteps played 710.5642361111111\n",
      "Time alive: [ 99.2 234.4  53.2  33.  157.  102.6 125.8 100.8 290.2  61.6]\n",
      "Avg time alive: 174.06944444444446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████████████████████████████████▍           | 420000/500000 [5:35:18<59:29, 22.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[64 49 57 56 55 54 53 52 51 48]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 973 1073  530  265  665  567  838  645 1684  618]\n",
      "avg (accross all scenarios) number of timsteps played 728.0381944444445\n",
      "Time alive: [194.6 214.6 106.   53.  133.  113.4 167.6 129.  336.8 123.6]\n",
      "Avg time alive: 176.01718749999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████████████████████████████████▉          | 429999/500000 [5:43:19<51:20, 22.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[98 55 82 81 80 79 78 77 75 74]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[1555  665 1095  235 1708 1165 1849 1482 1403  944]\n",
      "avg (accross all scenarios) number of timsteps played 746.3819444444445\n",
      "Time alive: [311.  133.  219.   47.  341.6 233.  369.8 296.4 280.6 188.8]\n",
      "Avg time alive: 178.0376736111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████▎        | 440000/500000 [5:51:17<43:34, 22.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[106  97  91  92  93  94  95  96  98  89]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[ 383 2105  786  977  164  500  694 1104 1555  591]\n",
      "avg (accross all scenarios) number of timsteps played 763.7309027777778\n",
      "Time alive: [ 76.6 421.  157.2 195.4  32.8 100.  138.8 220.8 311.  118.2]\n",
      "Avg time alive: 179.69140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████▊       | 450000/500000 [5:59:19<37:32, 22.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 113 116 117 118 119 120 121 122 123]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[2117 1401 2104  939 1313 1386  981 1347  983  741]\n",
      "avg (accross all scenarios) number of timsteps played 780.6736111111111\n",
      "Time alive: [423.4 280.2 420.8 187.8 262.6 277.2 196.2 269.4 196.6 148.2]\n",
      "Avg time alive: 180.99618055555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████████████▏     | 459999/500000 [6:07:18<30:04, 22.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 135 137 138 139 140 141 142 144 145]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[2117  805 1196  428  850 1459 1068  420  864  824]\n",
      "avg (accross all scenarios) number of timsteps played 798.5364583333334\n",
      "Time alive: [423.4 161.  239.2  85.6 170.  291.8 213.6  84.  172.8 164.8]\n",
      "Avg time alive: 182.83055555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████████████████████████████████▋    | 470000/500000 [6:15:22<22:22, 22.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 155 157 158 159 160 161 162 163 164]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[2117  325  659 1886 2171  617 1385 1234  173  368]\n",
      "avg (accross all scenarios) number of timsteps played 815.2690972222222\n",
      "Time alive: [423.4  65.  131.8 377.2 434.2 123.4 277.  246.8  34.6  73.6]\n",
      "Avg time alive: 184.35451388888887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████████████████████████████████████████████████████████████████   | 479998/500000 [6:23:21<14:39, 22.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[143 183 189 188 187 186 185 184 182 174]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[2117  957  936  404  855  838 1201 1147 2140 1341]\n",
      "avg (accross all scenarios) number of timsteps played 833.1232638888889\n",
      "Time alive: [423.4 191.4 187.2  80.8 171.  167.6 240.2 229.4 428.  268.2]\n",
      "Avg time alive: 186.0440972222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████▌ | 490000/500000 [6:31:24<08:18, 20.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardest scenarios\n",
      "[287 203 210 209 208 207 206 205 204 202]\n",
      "They have been chosen respectively\n",
      "[4 4 4 4 4 4 4 4 4 4]\n",
      "The number of timesteps played is\n",
      "[1233  568 1035  586  793 1333  748 1190  448  404]\n",
      "avg (accross all scenarios) number of timsteps played 850.4079861111111\n",
      "Time alive: [246.6 113.6 207.  117.2 158.6 266.6 149.6 238.   89.6  80.8]\n",
      "Avg time alive: 187.20026041666668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 500000/500000 [6:39:28<00:00, 20.86it/s]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "env = grid2op.make(\"l2rpn_neurips_2020_track1_small\", reward_class=L2RPNReward)\n",
    "tp = TrainingParam()\n",
    "\n",
    "li_attr_obs_X = [\"day_of_week\", \"hour_of_day\", \"minute_of_hour\", \"prod_p\", \"prod_v\", \"load_p\", \"load_q\",\n",
    "                         \"actual_dispatch\", \"target_dispatch\", \"topo_vect\", \"time_before_cooldown_line\",\n",
    "                         \"time_before_cooldown_sub\", \"rho\", \"timestep_overflow\", \"line_status\"]\n",
    "\n",
    "observation_size = NNParam.get_obs_size(env, li_attr_obs_X)\n",
    "sizes = [800, 800, 800, 494, 494, 494]  # sizes of each hidden layers\n",
    "kwargs_archi = {'observation_size': observation_size,\n",
    "                        'sizes': sizes,\n",
    "                        'activs': [\"relu\" for _ in sizes],  # all relu activation function\n",
    "                        \"list_attr_obs\": li_attr_obs_X}\n",
    "\n",
    "kwargs_converters = {\"all_actions\": None,\n",
    "                             \"set_line_status\": False,\n",
    "                             \"change_bus_vect\": True,\n",
    "                             \"set_topo_vect\": False\n",
    "                             }\n",
    "# define the name of the model\n",
    "nm_ = \"Deep_SARSA_Agent\"\n",
    "try:\n",
    "    train(env,\n",
    "          name=nm_,\n",
    "          iterations=500000,\n",
    "          save_path=\"./D_SARSA_Agent/model\",\n",
    "          load_path=None,\n",
    "          logs_dir=\"./D_SARSA_Agent/logs\",\n",
    "          training_param=tp,\n",
    "          kwargs_converters=kwargs_converters,\n",
    "          kwargs_archi=kwargs_archi)\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19160ae",
   "metadata": {},
   "source": [
    "## Below code snippet evaluates the saved agent and returns the requested data\n",
    "If ``verbose= True`` then a detailed log of the process is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c46e0a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.DeepQSimple at 0x1ca670fcaf0>,\n",
       " [('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_000',\n",
       "   'Scenario_april_000',\n",
       "   8005.5439453125,\n",
       "   155,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_001',\n",
       "   'Scenario_april_001',\n",
       "   18405.48046875,\n",
       "   361,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_002',\n",
       "   'Scenario_april_002',\n",
       "   39478.53125,\n",
       "   788,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_003',\n",
       "   'Scenario_april_003',\n",
       "   41038.21875,\n",
       "   796,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_004',\n",
       "   'Scenario_april_004',\n",
       "   8985.732421875,\n",
       "   173,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_005',\n",
       "   'Scenario_april_005',\n",
       "   4870.7607421875,\n",
       "   95,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_006',\n",
       "   'Scenario_april_006',\n",
       "   27276.99609375,\n",
       "   528,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_007',\n",
       "   'Scenario_april_007',\n",
       "   47906.90625,\n",
       "   951,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_008',\n",
       "   'Scenario_april_008',\n",
       "   8183.35986328125,\n",
       "   159,\n",
       "   8062),\n",
       "  ('C:\\\\Users\\\\tejus_\\\\data_grid2op\\\\l2rpn_neurips_2020_track1_small\\\\chronics\\\\Scenario_april_009',\n",
       "   'Scenario_april_009',\n",
       "   13484.7734375,\n",
       "   260,\n",
       "   8062)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = grid2op.make(\"l2rpn_neurips_2020_track1_small\", reward_class=L2RPNReward)\n",
    "evaluate(env,\n",
    "         name=\"Deep_SARSA_Agent\",\n",
    "         load_path=\"./D_SARSA_Agent/model\",\n",
    "         logs_path=\"./D_SARSA_Agent/logs\",\n",
    "         nb_episode=10,\n",
    "         nb_process=1,\n",
    "         max_steps=-1,\n",
    "         verbose=False,\n",
    "         save_gif=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411075bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
